{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP20TWp+5Ve6TDxggx4QvmG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tchaase/cVAE_autism/blob/main/code/cVAE_autism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contrastive Variational Autoencoder for the ABIDE Data Set\n",
        "\n",
        "Author - Tobias Haase"
      ],
      "metadata": {
        "id": "JrGziUQyNza-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "Firstly I am importaing the necessary modules here, that I will use within the following.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2OjCHUXODgN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kHlOG4l2NsXd"
      },
      "outputs": [],
      "source": [
        "import torch  # The main PyTorch library for tensor computations and neural network operations\n",
        "\n",
        "import torch.nn as nn  # Provides various neural network layers and functionalities\n",
        "import torch.nn.functional as F  # Provides functional interfaces to common operations (e.g., activation functions)\n",
        "import torch.optim as optim  # Contains various optimization algorithms (e.g., SGD, Adam)\n",
        "\n",
        "import torchvision  # A PyTorch library for computer vision tasks\n",
        "import torchvision.transforms as transforms  # Provides common image transformations (e.g., resizing, normalization)\n",
        "from torchvision.transforms import ToTensor  # Transforms PIL images to tensors\n",
        "from torch.utils.data import Dataset, DataLoader  # Provides tools for creating custom datasets and data loaders\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np  # NumPy library for numerical computations and array operations\n",
        "import matplotlib  # Matplotlib library for data visualization\n",
        "import matplotlib.pyplot as plt  # Matplotlib's pyplot module for creating plots\n",
        "from tqdm import tqdm  # Progress bar library for tracking iterations\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "gqOfbkF5lw1I",
        "outputId": "e5216119-7925-4b8a-c385-99f03bba4128",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, it is very helpful to keep a set seed so my analyes are reproducible."
      ],
      "metadata": {
        "id": "X8oZ4pZ2CVde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "seed_value = 42\n",
        "\n",
        "# Set seed for Python's random module\n",
        "random.seed(seed_value)\n",
        "\n",
        "# Set seed for NumPy\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# Set seed for PyTorch (CPU and CUDA)\n",
        "torch.manual_seed(seed_value)\n",
        "torch.cuda.manual_seed(seed_value)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Now all random operations in Python, NumPy, and PyTorch will be reproducible"
      ],
      "metadata": {
        "id": "s88loeEKCZXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I am loading the project's data. To load the data there are multiple options - one is cyberduck. I deviated from using cyberduck as it was easier to figure out the links for other files via `urllib.request.urlretrieve`. However I am leaving the code here - but commenting it out.\n",
        "\n",
        "Firstly, to use **CyberDuck**, one would need to install it:\n"
      ],
      "metadata": {
        "id": "k_7TkkoWPkfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!echo -e \"deb https://s3.amazonaws.com/repo.deb.cyberduck.io stable main\" | sudo tee /etc/apt/sources.list.d/cyberduck.list > /dev/null\n",
        "#!sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys FE7097963FEFBE72\n",
        "#!sudo apt-get update\n",
        "#!sudo apt-get install duck\n",
        "\n",
        "#duck --username anonymous --verbose --download s3:/fcp-indi/data/Projects/ABIDE_Initiative/Outputs/ants/anat_thickness/*_anat_thickness.nii.gz ./drive/MyDrive/MasterThesisData"
      ],
      "metadata": {
        "id": "S_EPD_35JAlp",
        "outputId": "7f3d6cf6-d3eb-46a5-f0ae-9452b0e651fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "Executing: /tmp/apt-key-gpghome.UDxYlqxJL7/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys FE7097963FEFBE72\n",
            "gpg: key F7FAE1F32DA69515: public key \"Cyberduck <feedback@cyberduck.io>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:6 https://s3.amazonaws.com/repo.deb.cyberduck.io stable InRelease [3,245 B]\n",
            "Get:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://s3.amazonaws.com/repo.deb.cyberduck.io stable/main amd64 Packages [370 B]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [979 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,236 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,163 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [840 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,110 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [862 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.8 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [876 kB]\n",
            "Get:21 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,110 kB]\n",
            "Fetched 9,589 kB in 5s (1,896 kB/s)\n",
            "Reading package lists... Done\n",
            "W: https://s3.amazonaws.com/repo.deb.cyberduck.io/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  duck\n",
            "0 upgraded, 1 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 88.9 MB of archives.\n",
            "After this operation, 212 MB of additional disk space will be used.\n",
            "Get:1 https://s3.amazonaws.com/repo.deb.cyberduck.io stable/main amd64 duck amd64 8.6.3.40040 [88.9 MB]\n",
            "Fetched 88.9 MB in 1s (68.8 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package duck.\n",
            "(Reading database ... 120828 files and directories currently installed.)\n",
            "Preparing to unpack .../duck_8.6.3.40040_amd64.deb ...\n",
            "Unpacking duck (8.6.3.40040) ...\n",
            "Setting up duck (8.6.3.40040) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As I want to use a specific atlas, I need to do the transformations myself. For this, I need access to the ANTs pipeline. Thankfully you can load it like below, as explained [here]( https://colab.research.google.com/drive/1g5cnZxj1llRaHmOs4xSglqsXnFkQYuol?usp=sharing#scrollTo=7J9SVgWtVD_D)."
      ],
      "metadata": {
        "id": "zIeeBlwdsqGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"LD_PRELOAD\"] = \"\";\n",
        "os.environ[\"APPTAINER_BINDPATH\"] = \"/content\"\n",
        "os.environ[\"LMOD_CMD\"] = \"/usr/share/lmod/lmod/libexec/lmod\"\n",
        "\n",
        "!curl -J -O https://raw.githubusercontent.com/NeuroDesk/neurocommand/main/googlecolab_setup.sh\n",
        "!chmod +x googlecolab_setup.sh\n",
        "!./googlecolab_setup.sh\n",
        "os.environ[\"MODULEPATH\"] = ':'.join(map(str, list(map(lambda x: os.path.join(os.path.abspath('/cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/'), x),os.listdir('/cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/')))))"
      ],
      "metadata": {
        "id": "zY-vvMsQsuuv",
        "outputId": "e5c7df09-98ce-4146-d5f5-15d3b04441a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3338  100  3338    0     0  12427      0 --:--:-- --:--:-- --:--:-- 12455\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "lsb-release is already the newest version (11.1.0ubuntu4).\n",
            "lsb-release set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
            "--2023-08-25 22:17:05--  https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest_all.deb\n",
            "Resolving ecsft.cern.ch (ecsft.cern.ch)... 188.184.97.7\n",
            "Connecting to ecsft.cern.ch (ecsft.cern.ch)|188.184.97.7|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5174 (5.1K)\n",
            "Saving to: ‘cvmfs-release-latest_all.deb’\n",
            "\n",
            "cvmfs-release-lates 100%[===================>]   5.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-25 22:17:06 (186 MB/s) - ‘cvmfs-release-latest_all.deb’ saved [5174/5174]\n",
            "\n",
            "[DEBUG]: adding cfms repo\n",
            "[DEBUG]: apt-get update\n",
            "[DEBUG]: apt-get install cvmfs\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 24.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "--2023-08-25 22:18:11--  https://github.com/apptainer/apptainer/releases/download/v1.2.0/apptainer_1.2.0_amd64.deb\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/433446895/78450082-c3ff-4235-957e-876cce7cd13f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230825%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230825T221811Z&X-Amz-Expires=300&X-Amz-Signature=e976aaa70c02d071801706b884dcde1eae0a04fedcc4e83174dbe2927860a006&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=433446895&response-content-disposition=attachment%3B%20filename%3Dapptainer_1.2.0_amd64.deb&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-08-25 22:18:11--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/433446895/78450082-c3ff-4235-957e-876cce7cd13f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230825%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230825T221811Z&X-Amz-Expires=300&X-Amz-Signature=e976aaa70c02d071801706b884dcde1eae0a04fedcc4e83174dbe2927860a006&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=433446895&response-content-disposition=attachment%3B%20filename%3Dapptainer_1.2.0_amd64.deb&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25471036 (24M) [application/octet-stream]\n",
            "Saving to: ‘apptainer_1.2.0_amd64.deb’\n",
            "\n",
            "apptainer_1.2.0_amd 100%[===================>]  24.29M  75.1MB/s    in 0.3s    \n",
            "\n",
            "2023-08-25 22:18:12 (75.1 MB/s) - ‘apptainer_1.2.0_amd64.deb’ saved [25471036/25471036]\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'apptainer' instead of './apptainer_1.2.0_amd64.deb'\n",
            "The following additional packages will be installed:\n",
            "  fakeroot fuse-overlayfs fuse2fs libfakeroot liblzo2-2 libsquashfuse0\n",
            "  squashfs-tools squashfuse uidmap\n",
            "The following NEW packages will be installed:\n",
            "  apptainer fakeroot fuse-overlayfs fuse2fs libfakeroot liblzo2-2\n",
            "  libsquashfuse0 squashfs-tools squashfuse uidmap\n",
            "0 upgraded, 10 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 434 kB/25.9 MB of archives.\n",
            "After this operation, 118 MB of additional disk space will be used.\n",
            "Get:1 /content/apptainer_1.2.0_amd64.deb apptainer amd64 1.2.0 [25.5 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 uidmap amd64 1:4.8.1-2ubuntu2.1 [22.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsquashfuse0 amd64 0.1.103-3 [24.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 squashfuse amd64 0.1.103-3 [7,516 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 fuse2fs amd64 1.46.5-2ubuntu1.1 [30.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fuse-overlayfs amd64 1.7.1-1 [44.7 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfakeroot amd64 1.28-1ubuntu1 [31.5 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 fakeroot amd64 1.28-1ubuntu1 [60.4 kB]\n",
            "Fetched 434 kB in 1s (593 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 10.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package uidmap.\n",
            "(Reading database ... 122106 files and directories currently installed.)\n",
            "Preparing to unpack .../0-uidmap_1%3a4.8.1-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking uidmap (1:4.8.1-2ubuntu2.1) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../1-liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../2-squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Selecting previously unselected package libsquashfuse0:amd64.\n",
            "Preparing to unpack .../3-libsquashfuse0_0.1.103-3_amd64.deb ...\n",
            "Unpacking libsquashfuse0:amd64 (0.1.103-3) ...\n",
            "Selecting previously unselected package squashfuse.\n",
            "Preparing to unpack .../4-squashfuse_0.1.103-3_amd64.deb ...\n",
            "Unpacking squashfuse (0.1.103-3) ...\n",
            "Selecting previously unselected package fuse2fs.\n",
            "Preparing to unpack .../5-fuse2fs_1.46.5-2ubuntu1.1_amd64.deb ...\n",
            "Unpacking fuse2fs (1.46.5-2ubuntu1.1) ...\n",
            "Selecting previously unselected package fuse-overlayfs.\n",
            "Preparing to unpack .../6-fuse-overlayfs_1.7.1-1_amd64.deb ...\n",
            "Unpacking fuse-overlayfs (1.7.1-1) ...\n",
            "Selecting previously unselected package libfakeroot:amd64.\n",
            "Preparing to unpack .../7-libfakeroot_1.28-1ubuntu1_amd64.deb ...\n",
            "Unpacking libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
            "Selecting previously unselected package fakeroot.\n",
            "Preparing to unpack .../8-fakeroot_1.28-1ubuntu1_amd64.deb ...\n",
            "Unpacking fakeroot (1.28-1ubuntu1) ...\n",
            "Selecting previously unselected package apptainer.\n",
            "Preparing to unpack .../9-apptainer_1.2.0_amd64.deb ...\n",
            "Unpacking apptainer (1.2.0) ...\n",
            "Setting up uidmap (1:4.8.1-2ubuntu2.1) ...\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
            "Setting up fakeroot (1.28-1ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode\n",
            "Setting up fuse2fs (1.46.5-2ubuntu1.1) ...\n",
            "Setting up fuse-overlayfs (1.7.1-1) ...\n",
            "Setting up libsquashfuse0:amd64 (0.1.103-3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up squashfuse (0.1.103-3) ...\n",
            "Setting up apptainer (1.2.0) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "--2023-08-25 22:18:21--  https://github.com/apptainer/apptainer/releases/download/v1.2.0/apptainer-suid_1.2.0_amd64.deb\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/433446895/8be78007-3ae5-49ab-9e46-e8b82bcd219c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230825%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230825T221822Z&X-Amz-Expires=300&X-Amz-Signature=08cdb5e056da24bf2003c9bdc4b7cd7f6dc1838b2e71bb96e4c771216c914f96&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=433446895&response-content-disposition=attachment%3B%20filename%3Dapptainer-suid_1.2.0_amd64.deb&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-08-25 22:18:22--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/433446895/8be78007-3ae5-49ab-9e46-e8b82bcd219c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230825%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230825T221822Z&X-Amz-Expires=300&X-Amz-Signature=08cdb5e056da24bf2003c9bdc4b7cd7f6dc1838b2e71bb96e4c771216c914f96&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=433446895&response-content-disposition=attachment%3B%20filename%3Dapptainer-suid_1.2.0_amd64.deb&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5064744 (4.8M) [application/octet-stream]\n",
            "Saving to: ‘apptainer-suid_1.2.0_amd64.deb’\n",
            "\n",
            "apptainer-suid_1.2. 100%[===================>]   4.83M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-08-25 22:18:22 (77.4 MB/s) - ‘apptainer-suid_1.2.0_amd64.deb’ saved [5064744/5064744]\n",
            "\n",
            "Selecting previously unselected package apptainer-suid.\n",
            "(Reading database ... 122519 files and directories currently installed.)\n",
            "Preparing to unpack .../apptainer-suid_1.2.0_amd64.deb ...\n",
            "Unpacking apptainer-suid (1.2.0) ...\n",
            "Setting up apptainer-suid (1.2.0) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aria2 bind9-host bind9-libs git-annex git-remote-gcrypt libaria2-0\n",
            "  libc-ares2 libimagequant0 liblmdb0 libmaxminddb0 libraqm0 lua-bit32\n",
            "  lua-filesystem lua-json lua-lpeg lua-posix lua-term lua5.2 mailcap\n",
            "  mime-support netbase nocache patool python3-annexremote python3-appdirs\n",
            "  python3-argcomplete python3-boto python3-certifi python3-chardet\n",
            "  python3-datalad python3-deprecated python3-exif python3-exifread\n",
            "  python3-fasteners python3-github python3-gitlab python3-html5lib\n",
            "  python3-httpretty python3-humanize python3-idna python3-iso8601\n",
            "  python3-jsmin python3-keyrings.alt python3-mock python3-monotonic\n",
            "  python3-msgpack python3-mutagen python3-nacl python3-nose python3-olefile\n",
            "  python3-packaging python3-pbr python3-pil python3-pycryptodome\n",
            "  python3-pyperclip python3-requests python3-requests-toolbelt\n",
            "  python3-setuptools python3-simplejson python3-tqdm python3-urllib3\n",
            "  python3-vcr python3-webencodings python3-whoosh python3-wrapt python3-yaml\n",
            "Suggested packages:\n",
            "  datalad-container datalad-crawler datalad-neuroimaging xdot bup adb tor\n",
            "  magic-wormhole tahoe-lafs libnss-mdns uftp youtube-dl rclone mmdb-bin arj\n",
            "  cabextract | lcab ncompress cpio lzop rpm2cpio lha unace | unace-nonfree\n",
            "  | nomarch unalz lrzip lhasa xdms orange lzip | plzip | clzip | pdlzip\n",
            "  sharutils flac libarchive-tools archmage genisoimage python-argcomplete\n",
            "  python3-duecredit python3-bs4 python3-numpy python-gitlab-doc python3-genshi\n",
            "  python3-lxml python-mock-doc python-mutagen-doc python-nacl-doc\n",
            "  python-nose-doc python-pil-doc python3-openssl python3-socks\n",
            "  python-requests-doc python-setuptools-doc python-whoosh-doc\n",
            "Recommended packages:\n",
            "  python3-libxmp python3-lzma python3-requests-ftp\n",
            "The following NEW packages will be installed:\n",
            "  aria2 bind9-host bind9-libs datalad git-annex git-remote-gcrypt libaria2-0\n",
            "  libc-ares2 libimagequant0 liblmdb0 libmaxminddb0 libraqm0 lmod lua-bit32\n",
            "  lua-filesystem lua-json lua-lpeg lua-posix lua-term lua5.2 mailcap\n",
            "  mime-support netbase nocache patool python3-annexremote python3-appdirs\n",
            "  python3-argcomplete python3-boto python3-certifi python3-chardet\n",
            "  python3-datalad python3-deprecated python3-exif python3-exifread\n",
            "  python3-fasteners python3-github python3-gitlab python3-html5lib\n",
            "  python3-httpretty python3-humanize python3-idna python3-iso8601\n",
            "  python3-jsmin python3-keyrings.alt python3-mock python3-monotonic\n",
            "  python3-msgpack python3-mutagen python3-nacl python3-nose python3-olefile\n",
            "  python3-packaging python3-pbr python3-pil python3-pycryptodome\n",
            "  python3-pyperclip python3-requests python3-requests-toolbelt\n",
            "  python3-setuptools python3-simplejson python3-tqdm python3-urllib3\n",
            "  python3-vcr python3-webencodings python3-whoosh python3-wrapt python3-yaml\n",
            "0 upgraded, 68 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 21.9 MB of archives.\n",
            "After this operation, 122 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 netbase all 6.3 [12.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-yaml amd64 5.4.1-1ubuntu1 [129 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblmdb0 amd64 0.9.24-1build2 [47.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmaxminddb0 amd64 1.5.2-1build2 [24.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 bind9-libs amd64 1:9.18.12-0ubuntu0.22.04.2 [1,240 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 bind9-host amd64 1:9.18.12-0ubuntu0.22.04.2 [52.4 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-ares2 amd64 1.18.1-1ubuntu0.22.04.2 [45.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaria2-0 amd64 1.36.0-1 [1,086 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 aria2 amd64 1.36.0-1 [381 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 git-annex amd64 8.20210223-2ubuntu2 [12.8 MB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 patool all 1.12-5 [33.0 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-annexremote all 1.6.0-1 [16.7 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-appdirs all 1.4.4-2 [11.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-monotonic all 1.6-2 [5,732 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-fasteners all 0.14.1-2 [14.1 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-certifi all 2020.6.20-1 [150 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-chardet all 4.0.0-1 [98.0 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-idna all 3.3-1 [49.3 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-urllib3 all 1.26.5-1~exp1 [96.4 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-requests all 2.25.1+dfsg-2ubuntu0.1 [48.8 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-requests-toolbelt all 0.9.1-1 [38.0 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-gitlab all 1:2.10.1-2 [54.0 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-humanize all 4.0.0-1 [45.7 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-iso8601 all 1.0.2-1 [15.8 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-pycryptodome amd64 3.11.0+dfsg1-3build1 [1,027 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-keyrings.alt all 4.1.0-1 [17.6 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-setuptools all 59.6.0-1.2ubuntu0.22.04.1 [339 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-pbr all 5.8.0-0ubuntu1 [66.5 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-mock all 4.0.3-3 [60.4 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-msgpack amd64 1.0.3-1build1 [67.8 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 libimagequant0 amd64 2.17.0-1 [34.6 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 libraqm0 amd64 0.7.0-4ubuntu1 [11.7 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-pil amd64 9.0.1-1ubuntu0.1 [419 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-simplejson amd64 3.17.6-1build1 [54.7 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-tqdm all 4.57.0-2 [93.0 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-wrapt amd64 1.13.3-1build1 [34.0 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-packaging all 21.3-1 [30.7 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-datalad all 0.15.5-1 [830 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-argcomplete all 1.8.1-1.5 [27.2 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 datalad all 0.15.5-1 [92.6 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy/universe amd64 git-remote-gcrypt all 1.4-1 [13.9 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lua-filesystem amd64 1.8.0-2 [13.7 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lua-lpeg amd64 1.0.2-1 [31.4 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lua-json all 1.3.4-2 [29.3 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lua-bit32 amd64 5.3.0-3 [10.1 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lua-posix amd64 33.4.0-3 [75.2 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lua-term amd64 0.07-1 [7,224 B]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lua5.2 amd64 5.2.4-2 [129 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lmod amd64 6.6-0.4 [157 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 nocache amd64 1.1-1 [17.5 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-boto all 2.49.0-4 [741 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-deprecated all 1.2.13-2 [9,804 B]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-exifread all 2.3.2-1 [30.3 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-nacl amd64 1.5.0-2 [63.1 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-github all 1.55-1 [94.0 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-webencodings all 0.5.1-4 [11.8 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-html5lib all 1.1-3 [87.0 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-httpretty all 1.1.4-1 [24.0 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-jsmin all 3.0.0-1 [12.9 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-mutagen all 1.45.1-2 [135 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-nose all 1.3.7-8 [117 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-olefile all 0.46-3 [33.8 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-pyperclip all 1.8.2-2 [11.5 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-vcr all 4.0.2-1 [29.9 kB]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-whoosh all 2.7.4+git6-g9134ad92-5 [288 kB]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-exif all 2.3.2-1 [3,520 B]\n",
            "Fetched 21.9 MB in 2s (12.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 68.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package netbase.\n",
            "(Reading database ... 122524 files and directories currently installed.)\n",
            "Preparing to unpack .../00-netbase_6.3_all.deb ...\n",
            "Unpacking netbase (6.3) ...\n",
            "Selecting previously unselected package python3-yaml.\n",
            "Preparing to unpack .../01-python3-yaml_5.4.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking python3-yaml (5.4.1-1ubuntu1) ...\n",
            "Selecting previously unselected package liblmdb0:amd64.\n",
            "Preparing to unpack .../02-liblmdb0_0.9.24-1build2_amd64.deb ...\n",
            "Unpacking liblmdb0:amd64 (0.9.24-1build2) ...\n",
            "Selecting previously unselected package libmaxminddb0:amd64.\n",
            "Preparing to unpack .../03-libmaxminddb0_1.5.2-1build2_amd64.deb ...\n",
            "Unpacking libmaxminddb0:amd64 (1.5.2-1build2) ...\n",
            "Selecting previously unselected package bind9-libs:amd64.\n",
            "Preparing to unpack .../04-bind9-libs_1%3a9.18.12-0ubuntu0.22.04.2_amd64.deb ...\n",
            "Unpacking bind9-libs:amd64 (1:9.18.12-0ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package bind9-host.\n",
            "Preparing to unpack .../05-bind9-host_1%3a9.18.12-0ubuntu0.22.04.2_amd64.deb ...\n",
            "Unpacking bind9-host (1:9.18.12-0ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "Preparing to unpack .../06-libc-ares2_1.18.1-1ubuntu0.22.04.2_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../07-libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../08-aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Selecting previously unselected package git-annex.\n",
            "Preparing to unpack .../09-git-annex_8.20210223-2ubuntu2_amd64.deb ...\n",
            "Unpacking git-annex (8.20210223-2ubuntu2) ...\n",
            "Selecting previously unselected package patool.\n",
            "Preparing to unpack .../10-patool_1.12-5_all.deb ...\n",
            "Unpacking patool (1.12-5) ...\n",
            "Selecting previously unselected package python3-annexremote.\n",
            "Preparing to unpack .../11-python3-annexremote_1.6.0-1_all.deb ...\n",
            "Unpacking python3-annexremote (1.6.0-1) ...\n",
            "Selecting previously unselected package python3-appdirs.\n",
            "Preparing to unpack .../12-python3-appdirs_1.4.4-2_all.deb ...\n",
            "Unpacking python3-appdirs (1.4.4-2) ...\n",
            "Selecting previously unselected package python3-monotonic.\n",
            "Preparing to unpack .../13-python3-monotonic_1.6-2_all.deb ...\n",
            "Unpacking python3-monotonic (1.6-2) ...\n",
            "Selecting previously unselected package python3-fasteners.\n",
            "Preparing to unpack .../14-python3-fasteners_0.14.1-2_all.deb ...\n",
            "Unpacking python3-fasteners (0.14.1-2) ...\n",
            "Selecting previously unselected package python3-certifi.\n",
            "Preparing to unpack .../15-python3-certifi_2020.6.20-1_all.deb ...\n",
            "Unpacking python3-certifi (2020.6.20-1) ...\n",
            "Selecting previously unselected package python3-chardet.\n",
            "Preparing to unpack .../16-python3-chardet_4.0.0-1_all.deb ...\n",
            "Unpacking python3-chardet (4.0.0-1) ...\n",
            "Selecting previously unselected package python3-idna.\n",
            "Preparing to unpack .../17-python3-idna_3.3-1_all.deb ...\n",
            "Unpacking python3-idna (3.3-1) ...\n",
            "Selecting previously unselected package python3-urllib3.\n",
            "Preparing to unpack .../18-python3-urllib3_1.26.5-1~exp1_all.deb ...\n",
            "Unpacking python3-urllib3 (1.26.5-1~exp1) ...\n",
            "Selecting previously unselected package python3-requests.\n",
            "Preparing to unpack .../19-python3-requests_2.25.1+dfsg-2ubuntu0.1_all.deb ...\n",
            "Unpacking python3-requests (2.25.1+dfsg-2ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-requests-toolbelt.\n",
            "Preparing to unpack .../20-python3-requests-toolbelt_0.9.1-1_all.deb ...\n",
            "Unpacking python3-requests-toolbelt (0.9.1-1) ...\n",
            "Selecting previously unselected package python3-gitlab.\n",
            "Preparing to unpack .../21-python3-gitlab_1%3a2.10.1-2_all.deb ...\n",
            "Unpacking python3-gitlab (1:2.10.1-2) ...\n",
            "Selecting previously unselected package python3-humanize.\n",
            "Preparing to unpack .../22-python3-humanize_4.0.0-1_all.deb ...\n",
            "Unpacking python3-humanize (4.0.0-1) ...\n",
            "Selecting previously unselected package python3-iso8601.\n",
            "Preparing to unpack .../23-python3-iso8601_1.0.2-1_all.deb ...\n",
            "Unpacking python3-iso8601 (1.0.2-1) ...\n",
            "Selecting previously unselected package python3-pycryptodome.\n",
            "Preparing to unpack .../24-python3-pycryptodome_3.11.0+dfsg1-3build1_amd64.deb ...\n",
            "Unpacking python3-pycryptodome (3.11.0+dfsg1-3build1) ...\n",
            "Selecting previously unselected package python3-keyrings.alt.\n",
            "Preparing to unpack .../25-python3-keyrings.alt_4.1.0-1_all.deb ...\n",
            "Unpacking python3-keyrings.alt (4.1.0-1) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../26-python3-setuptools_59.6.0-1.2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pbr.\n",
            "Preparing to unpack .../27-python3-pbr_5.8.0-0ubuntu1_all.deb ...\n",
            "Unpacking python3-pbr (5.8.0-0ubuntu1) ...\n",
            "Selecting previously unselected package python3-mock.\n",
            "Preparing to unpack .../28-python3-mock_4.0.3-3_all.deb ...\n",
            "Unpacking python3-mock (4.0.3-3) ...\n",
            "Selecting previously unselected package python3-msgpack.\n",
            "Preparing to unpack .../29-python3-msgpack_1.0.3-1build1_amd64.deb ...\n",
            "Unpacking python3-msgpack (1.0.3-1build1) ...\n",
            "Selecting previously unselected package mailcap.\n",
            "Preparing to unpack .../30-mailcap_3.70+nmu1ubuntu1_all.deb ...\n",
            "Unpacking mailcap (3.70+nmu1ubuntu1) ...\n",
            "Selecting previously unselected package mime-support.\n",
            "Preparing to unpack .../31-mime-support_3.66_all.deb ...\n",
            "Unpacking mime-support (3.66) ...\n",
            "Selecting previously unselected package libimagequant0:amd64.\n",
            "Preparing to unpack .../32-libimagequant0_2.17.0-1_amd64.deb ...\n",
            "Unpacking libimagequant0:amd64 (2.17.0-1) ...\n",
            "Selecting previously unselected package libraqm0:amd64.\n",
            "Preparing to unpack .../33-libraqm0_0.7.0-4ubuntu1_amd64.deb ...\n",
            "Unpacking libraqm0:amd64 (0.7.0-4ubuntu1) ...\n",
            "Selecting previously unselected package python3-pil:amd64.\n",
            "Preparing to unpack .../34-python3-pil_9.0.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking python3-pil:amd64 (9.0.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-simplejson.\n",
            "Preparing to unpack .../35-python3-simplejson_3.17.6-1build1_amd64.deb ...\n",
            "Unpacking python3-simplejson (3.17.6-1build1) ...\n",
            "Selecting previously unselected package python3-tqdm.\n",
            "Preparing to unpack .../36-python3-tqdm_4.57.0-2_all.deb ...\n",
            "Unpacking python3-tqdm (4.57.0-2) ...\n",
            "Selecting previously unselected package python3-wrapt.\n",
            "Preparing to unpack .../37-python3-wrapt_1.13.3-1build1_amd64.deb ...\n",
            "Unpacking python3-wrapt (1.13.3-1build1) ...\n",
            "Selecting previously unselected package python3-packaging.\n",
            "Preparing to unpack .../38-python3-packaging_21.3-1_all.deb ...\n",
            "Unpacking python3-packaging (21.3-1) ...\n",
            "Selecting previously unselected package python3-datalad.\n",
            "Preparing to unpack .../39-python3-datalad_0.15.5-1_all.deb ...\n",
            "Unpacking python3-datalad (0.15.5-1) ...\n",
            "Selecting previously unselected package python3-argcomplete.\n",
            "Preparing to unpack .../40-python3-argcomplete_1.8.1-1.5_all.deb ...\n",
            "Unpacking python3-argcomplete (1.8.1-1.5) ...\n",
            "Selecting previously unselected package datalad.\n",
            "Preparing to unpack .../41-datalad_0.15.5-1_all.deb ...\n",
            "Unpacking datalad (0.15.5-1) ...\n",
            "Selecting previously unselected package git-remote-gcrypt.\n",
            "Preparing to unpack .../42-git-remote-gcrypt_1.4-1_all.deb ...\n",
            "Unpacking git-remote-gcrypt (1.4-1) ...\n",
            "Selecting previously unselected package lua-filesystem:amd64.\n",
            "Preparing to unpack .../43-lua-filesystem_1.8.0-2_amd64.deb ...\n",
            "Unpacking lua-filesystem:amd64 (1.8.0-2) ...\n",
            "Selecting previously unselected package lua-lpeg:amd64.\n",
            "Preparing to unpack .../44-lua-lpeg_1.0.2-1_amd64.deb ...\n",
            "Unpacking lua-lpeg:amd64 (1.0.2-1) ...\n",
            "Selecting previously unselected package lua-json.\n",
            "Preparing to unpack .../45-lua-json_1.3.4-2_all.deb ...\n",
            "Unpacking lua-json (1.3.4-2) ...\n",
            "Selecting previously unselected package lua-bit32:amd64.\n",
            "Preparing to unpack .../46-lua-bit32_5.3.0-3_amd64.deb ...\n",
            "Unpacking lua-bit32:amd64 (5.3.0-3) ...\n",
            "Selecting previously unselected package lua-posix:amd64.\n",
            "Preparing to unpack .../47-lua-posix_33.4.0-3_amd64.deb ...\n",
            "Unpacking lua-posix:amd64 (33.4.0-3) ...\n",
            "Selecting previously unselected package lua-term.\n",
            "Preparing to unpack .../48-lua-term_0.07-1_amd64.deb ...\n",
            "Unpacking lua-term (0.07-1) ...\n",
            "Selecting previously unselected package lua5.2.\n",
            "Preparing to unpack .../49-lua5.2_5.2.4-2_amd64.deb ...\n",
            "Unpacking lua5.2 (5.2.4-2) ...\n",
            "Selecting previously unselected package lmod.\n",
            "Preparing to unpack .../50-lmod_6.6-0.4_amd64.deb ...\n",
            "Unpacking lmod (6.6-0.4) ...\n",
            "Selecting previously unselected package nocache.\n",
            "Preparing to unpack .../51-nocache_1.1-1_amd64.deb ...\n",
            "Unpacking nocache (1.1-1) ...\n",
            "Selecting previously unselected package python3-boto.\n",
            "Preparing to unpack .../52-python3-boto_2.49.0-4_all.deb ...\n",
            "Unpacking python3-boto (2.49.0-4) ...\n",
            "Selecting previously unselected package python3-deprecated.\n",
            "Preparing to unpack .../53-python3-deprecated_1.2.13-2_all.deb ...\n",
            "Unpacking python3-deprecated (1.2.13-2) ...\n",
            "Selecting previously unselected package python3-exifread.\n",
            "Preparing to unpack .../54-python3-exifread_2.3.2-1_all.deb ...\n",
            "Unpacking python3-exifread (2.3.2-1) ...\n",
            "Selecting previously unselected package python3-nacl.\n",
            "Preparing to unpack .../55-python3-nacl_1.5.0-2_amd64.deb ...\n",
            "Unpacking python3-nacl (1.5.0-2) ...\n",
            "Selecting previously unselected package python3-github.\n",
            "Preparing to unpack .../56-python3-github_1.55-1_all.deb ...\n",
            "Unpacking python3-github (1.55-1) ...\n",
            "Selecting previously unselected package python3-webencodings.\n",
            "Preparing to unpack .../57-python3-webencodings_0.5.1-4_all.deb ...\n",
            "Unpacking python3-webencodings (0.5.1-4) ...\n",
            "Selecting previously unselected package python3-html5lib.\n",
            "Preparing to unpack .../58-python3-html5lib_1.1-3_all.deb ...\n",
            "Unpacking python3-html5lib (1.1-3) ...\n",
            "Selecting previously unselected package python3-httpretty.\n",
            "Preparing to unpack .../59-python3-httpretty_1.1.4-1_all.deb ...\n",
            "Unpacking python3-httpretty (1.1.4-1) ...\n",
            "Selecting previously unselected package python3-jsmin.\n",
            "Preparing to unpack .../60-python3-jsmin_3.0.0-1_all.deb ...\n",
            "Unpacking python3-jsmin (3.0.0-1) ...\n",
            "Selecting previously unselected package python3-mutagen.\n",
            "Preparing to unpack .../61-python3-mutagen_1.45.1-2_all.deb ...\n",
            "Unpacking python3-mutagen (1.45.1-2) ...\n",
            "Selecting previously unselected package python3-nose.\n",
            "Preparing to unpack .../62-python3-nose_1.3.7-8_all.deb ...\n",
            "Unpacking python3-nose (1.3.7-8) ...\n",
            "Selecting previously unselected package python3-olefile.\n",
            "Preparing to unpack .../63-python3-olefile_0.46-3_all.deb ...\n",
            "Unpacking python3-olefile (0.46-3) ...\n",
            "Selecting previously unselected package python3-pyperclip.\n",
            "Preparing to unpack .../64-python3-pyperclip_1.8.2-2_all.deb ...\n",
            "Unpacking python3-pyperclip (1.8.2-2) ...\n",
            "Selecting previously unselected package python3-vcr.\n",
            "Preparing to unpack .../65-python3-vcr_4.0.2-1_all.deb ...\n",
            "Unpacking python3-vcr (4.0.2-1) ...\n",
            "Selecting previously unselected package python3-whoosh.\n",
            "Preparing to unpack .../66-python3-whoosh_2.7.4+git6-g9134ad92-5_all.deb ...\n",
            "Unpacking python3-whoosh (2.7.4+git6-g9134ad92-5) ...\n",
            "Selecting previously unselected package python3-exif.\n",
            "Preparing to unpack .../67-python3-exif_2.3.2-1_all.deb ...\n",
            "Unpacking python3-exif (2.3.2-1) ...\n",
            "Setting up liblmdb0:amd64 (0.9.24-1build2) ...\n",
            "Setting up lua-lpeg:amd64 (1.0.2-1) ...\n",
            "Setting up patool (1.12-5) ...\n",
            "Setting up python3-annexremote (1.6.0-1) ...\n",
            "Setting up git-remote-gcrypt (1.4-1) ...\n",
            "Setting up python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Setting up python3-exifread (2.3.2-1) ...\n",
            "Setting up python3-pbr (5.8.0-0ubuntu1) ...\n",
            "Setting up lua5.2 (5.2.4-2) ...\n",
            "update-alternatives: using /usr/bin/lua5.2 to provide /usr/bin/lua (lua-interpreter) in auto mode\n",
            "update-alternatives: using /usr/bin/luac5.2 to provide /usr/bin/luac (lua-compiler) in auto mode\n",
            "Setting up libmaxminddb0:amd64 (1.5.2-1build2) ...\n",
            "Setting up python3-pycryptodome (3.11.0+dfsg1-3build1) ...\n",
            "Setting up python3-olefile (0.46-3) ...\n",
            "Setting up python3-tqdm (4.57.0-2) ...\n",
            "Setting up python3-mutagen (1.45.1-2) ...\n",
            "Setting up lua-term (0.07-1) ...\n",
            "Setting up lua-json (1.3.4-2) ...\n",
            "Setting up python3-yaml (5.4.1-1ubuntu1) ...\n",
            "Setting up python3-pyperclip (1.8.2-2) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.2) ...\n",
            "Setting up python3-nose (1.3.7-8) ...\n",
            "Setting up python3-webencodings (0.5.1-4) ...\n",
            "Setting up python3-monotonic (1.6-2) ...\n",
            "Setting up python3-mock (4.0.3-3) ...\n",
            "Setting up python3-simplejson (3.17.6-1build1) ...\n",
            "Setting up lua-filesystem:amd64 (1.8.0-2) ...\n",
            "Setting up lua-bit32:amd64 (5.3.0-3) ...\n",
            "Setting up python3-packaging (21.3-1) ...\n",
            "Setting up python3-iso8601 (1.0.2-1) ...\n",
            "Setting up python3-chardet (4.0.0-1) ...\n",
            "Setting up nocache (1.1-1) ...\n",
            "Setting up python3-jsmin (3.0.0-1) ...\n",
            "Setting up python3-certifi (2020.6.20-1) ...\n",
            "Setting up python3-exif (2.3.2-1) ...\n",
            "Setting up libraqm0:amd64 (0.7.0-4ubuntu1) ...\n",
            "Setting up libimagequant0:amd64 (2.17.0-1) ...\n",
            "Setting up python3-idna (3.3-1) ...\n",
            "Setting up python3-wrapt (1.13.3-1build1) ...\n",
            "Setting up python3-vcr (4.0.2-1) ...\n",
            "Setting up python3-fasteners (0.14.1-2) ...\n",
            "Setting up python3-html5lib (1.1-3) ...\n",
            "Setting up python3-urllib3 (1.26.5-1~exp1) ...\n",
            "Setting up python3-keyrings.alt (4.1.0-1) ...\n",
            "Setting up netbase (6.3) ...\n",
            "Setting up python3-msgpack (1.0.3-1build1) ...\n",
            "Setting up python3-argcomplete (1.8.1-1.5) ...\n",
            "Setting up python3-humanize (4.0.0-1) ...\n",
            "Setting up python3-appdirs (1.4.4-2) ...\n",
            "Setting up mailcap (3.70+nmu1ubuntu1) ...\n",
            "Setting up python3-whoosh (2.7.4+git6-g9134ad92-5) ...\n",
            "/usr/lib/python3/dist-packages/whoosh/codec/whoosh3.py:1116: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  elif fixedsize is 0:\n",
            "Setting up python3-deprecated (1.2.13-2) ...\n",
            "Setting up python3-nacl (1.5.0-2) ...\n",
            "Setting up python3-httpretty (1.1.4-1) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up mime-support (3.66) ...\n",
            "Setting up bind9-libs:amd64 (1:9.18.12-0ubuntu0.22.04.2) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Setting up lua-posix:amd64 (33.4.0-3) ...\n",
            "Setting up python3-pil:amd64 (9.0.1-1ubuntu0.1) ...\n",
            "Setting up python3-requests (2.25.1+dfsg-2ubuntu0.1) ...\n",
            "Setting up git-annex (8.20210223-2ubuntu2) ...\n",
            "Setting up bind9-host (1:9.18.12-0ubuntu0.22.04.2) ...\n",
            "Setting up python3-boto (2.49.0-4) ...\n",
            "Setting up python3-requests-toolbelt (0.9.1-1) ...\n",
            "Setting up lmod (6.6-0.4) ...\n",
            "Setting up python3-github (1.55-1) ...\n",
            "Setting up python3-gitlab (1:2.10.1-2) ...\n",
            "Setting up python3-datalad (0.15.5-1) ...\n",
            "Setting up datalad (0.15.5-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Collecting jupyterlmod\n",
            "  Downloading jupyterlmod-4.0.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Collecting nilearn\n",
            "  Downloading nilearn-0.10.1-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Collecting nipype\n",
            "  Downloading nipype-1.8.6-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from jupyterlmod) (5.3.1)\n",
            "Requirement already satisfied: jupyter-server in /usr/local/lib/python3.10/dist-packages (from jupyterlmod) (1.24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.3.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nilearn) (4.9.3)\n",
            "Requirement already satisfied: nibabel>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (4.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nilearn) (23.1)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from nipype) (8.1.7)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.10/dist-packages (from nipype) (3.1)\n",
            "Collecting prov>=1.5.2 (from nipype)\n",
            "  Downloading prov-2.0.0-py3-none-any.whl (421 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from nipype) (1.4.2)\n",
            "Collecting rdflib>=5.0.0 (from nipype)\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: simplejson>=3.8.0 in /usr/lib/python3/dist-packages (from nipype) (3.17.6)\n",
            "Collecting traits!=5.0,<6.4,>=4.6 (from nipype)\n",
            "  Downloading traits-6.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from nipype) (3.12.2)\n",
            "Collecting etelemetry>=0.2.0 (from nipype)\n",
            "  Downloading etelemetry-0.3.0-py3-none-any.whl (6.3 kB)\n",
            "Collecting looseversion (from nipype)\n",
            "  Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting ci-info>=0.2 (from etelemetry>=0.2.0->nipype)\n",
            "  Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel>=3.2.0->nilearn) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=5.0.0->nipype)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2023.7.22)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->nilearn) (3.2.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->jupyterlmod) (3.10.0)\n",
            "Requirement already satisfied: traitlets>=5.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->jupyterlmod) (5.7.1)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server->jupyterlmod) (3.7.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from jupyter-server->jupyterlmod) (23.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from jupyter-server->jupyterlmod) (3.1.2)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server->jupyterlmod) (6.1.12)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server->jupyterlmod) (6.5.4)\n",
            "Requirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server->jupyterlmod) (5.9.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server->jupyterlmod) (0.17.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from jupyter-server->jupyterlmod) (23.2.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.10/dist-packages (from jupyter-server->jupyterlmod) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server->jupyterlmod) (0.17.1)\n",
            "Requirement already satisfied: tornado>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server->jupyterlmod) (6.3.2)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server->jupyterlmod) (1.6.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server->jupyterlmod) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server->jupyterlmod) (1.1.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server->jupyterlmod) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server->jupyterlmod) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server->jupyterlmod) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server->jupyterlmod) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server->jupyterlmod) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server->jupyterlmod) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server->jupyterlmod) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server->jupyterlmod) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server->jupyterlmod) (1.5.0)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server->jupyterlmod) (2.16.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server->jupyterlmod) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server->jupyterlmod) (2.18.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server->jupyterlmod) (4.19.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->jupyter-server->jupyterlmod) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->jupyter-server->jupyterlmod) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server->jupyterlmod) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server->jupyterlmod) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server->jupyterlmod) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server->jupyterlmod) (0.9.2)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server->jupyterlmod) (1.15.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server->jupyterlmod) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server->jupyterlmod) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server->jupyterlmod) (2.21)\n",
            "Installing collected packages: looseversion, traits, isodate, ci-info, rdflib, etelemetry, prov, nilearn, nipype, jupyterlmod\n",
            "Successfully installed ci-info-0.3.0 etelemetry-0.3.0 isodate-0.6.1 jupyterlmod-4.0.3 looseversion-1.3.0 nilearn-0.10.1 nipype-1.8.6 prov-2.0.0 rdflib-7.0.0 traits-6.3.2\n",
            "-----BEGIN PUBLIC KEY-----\n",
            "MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAwUPEmxDp217SAtZxaBep\n",
            "Bi2TQcLoh5AJ//HSIz68ypjOGFjwExGlHb95Frhu1SpcH5OASbV+jJ60oEBLi3sD\n",
            "qA6rGYt9kVi90lWvEjQnhBkPb0uWcp1gNqQAUocybCzHvoiG3fUzAe259CrK09qR\n",
            "pX8sZhgK3eHlfx4ycyMiIQeg66AHlgVCJ2fKa6fl1vnh6adJEPULmn6vZnevvUke\n",
            "I6U1VcYTKm5dPMrOlY/fGimKlyWvivzVv1laa5TAR2Dt4CfdQncOz+rkXmWjLjkD\n",
            "87WMiTgtKybsmMLb2yCGSgLSArlSWhbMA0MaZSzAwE9PJKCCMvTANo5644zc8jBe\n",
            "NQIDAQAB\n",
            "-----END PUBLIC KEY-----\n",
            "CVMFS_USE_GEOAPI=yes\n",
            "CVMFS_SERVER_URL=\"http://cvmfs.neurodesk.org/cvmfs/@fqrn@\"\n",
            "CVMFS_KEYS_DIR=\"/etc/cvmfs/keys/ardc.edu.au/\"\n",
            "CVMFS_HTTP_PROXY=DIRECT\n",
            "CVMFS_QUOTA_LIMIT=5000\n",
            "System has not been booted with systemd as init system (PID 1). Can't operate.\n",
            "Failed to connect to bus: Host is down\n",
            " * Stopping automount...\n",
            "   ...done.\n",
            "CernVM-FS: running with credentials 106:107\n",
            "CernVM-FS: loading Fuse module... done\n",
            "CernVM-FS: mounted cvmfs on /cvmfs/neurodesk.ardc.edu.au\n",
            "Warning: autofs service is not running\n",
            "Probing /cvmfs/neurodesk.ardc.edu.au... OK\n",
            "containers  neurodesk-modules\n",
            "Version: 2.10.1.0\n",
            "PID: 5358\n",
            "Uptime: 0 minutes\n",
            "Memory Usage: 23020k\n",
            "File Catalog Revision: 991 (expires in 3 minutes)\n",
            "File Catalog ID: 42829a33dbfff01c4601f494798d40191ae3f2da\n",
            "No. Active File Catalogs: 1\n",
            "Cache Usage: 7910k / 5120001k\n",
            "File Descriptor Usage: 0 / 130560\n",
            "No. Open Directories: 0\n",
            "No. IO Errors: 0\n",
            "Connection: http://cvmfs.neurodesk.org/cvmfs/neurodesk.ardc.edu.au through proxy DIRECT (online)\n",
            "Usage: 0 open() calls (hitrate 0.000%), 5 opendir() calls\n",
            "Transfer Statistics: 3993k read, avg. speed: 3993k/s\n",
            "OK\n",
            "  [0] http://cvmfs.neurodesk.org/cvmfs/neurodesk.ardc.edu.au (62 ms)\n",
            "Active host 0: http://cvmfs.neurodesk.org/cvmfs/neurodesk.ardc.edu.au\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I just load a version of ants!"
      ],
      "metadata": {
        "id": "39Rlbm6exF7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lmod\n",
        "await lmod.load('ants/2.3.5')"
      ],
      "metadata": {
        "id": "LyXyCgCAsxah"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I am mounting the drive so I can use it later to store the data."
      ],
      "metadata": {
        "id": "dVE0ilzIs2tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls ./data/anat_thickness/\n",
        "!rm -rf ./data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "3NoZwDU4dJ41",
        "outputId": "169d9bfc-94b3-45fa-c980-4ac386e0b785",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, its helpful to have the file with the participant info already."
      ],
      "metadata": {
        "id": "QOmEM1rpbdES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL to download the CSV file!\n",
        "csv_url = \"https://s3.amazonaws.com/fcp-indi/data/Projects/ABIDE_Initiative/Phenotypic_V1_0b_preprocessed1.csv\"  # Replace with the actual URL\n",
        "\n",
        "# Directory to store the CSV file\n",
        "data_directory = \"./data/participant_info\"\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(data_directory, exist_ok=True)\n",
        "\n",
        "# File path to save the CSV file\n",
        "csv_file_path = os.path.join(data_directory, \"participant_info.csv\")\n",
        "\n",
        "# Download the CSV file\n",
        "response = requests.get(csv_url)\n",
        "if response.status_code == 200:\n",
        "    with open(csv_file_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(\"CSV file downloaded successfully.\")\n",
        "else:\n",
        "    print(\"Failed to download the CSV file.\")\n"
      ],
      "metadata": {
        "id": "pRY_50GRbfow",
        "outputId": "559e4557-cf4c-4b02-f095-38700627ac6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the data transformed later I will also need a template. This is loaded here!"
      ],
      "metadata": {
        "id": "PdeMBUHhts4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install templateflow\n",
        "from templateflow import api as tflow\n",
        "mni152 = tflow.get('MNI152NLin2009cAsym', desc=None, resolution=1,\n",
        "                    suffix='T1w', extension='nii.gz')\n",
        "mni152_path = str(mni152)"
      ],
      "metadata": {
        "id": "c9-U1pcEtvTf",
        "outputId": "102433f4-4412-4689-b4ce-e29a333e62d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting templateflow\n",
            "  Downloading templateflow-23.0.0-py3-none-any.whl (455 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.4/455.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybids>=0.15.2 (from templateflow)\n",
            "  Downloading pybids-0.16.3-py3-none-any.whl (14.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from templateflow) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from templateflow) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from pybids>=0.15.2->templateflow) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from pybids>=0.15.2->templateflow) (1.10.1)\n",
            "Requirement already satisfied: nibabel>=3.0 in /usr/local/lib/python3.10/dist-packages (from pybids>=0.15.2->templateflow) (4.0.2)\n",
            "Requirement already satisfied: pandas>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from pybids>=0.15.2->templateflow) (1.5.3)\n",
            "Collecting formulaic<0.6,>=0.2.4 (from pybids>=0.15.2->templateflow)\n",
            "  Downloading formulaic-0.5.2-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.3.16 in /usr/local/lib/python3.10/dist-packages (from pybids>=0.15.2->templateflow) (2.0.20)\n",
            "Collecting bids-validator>=1.11 (from pybids>=0.15.2->templateflow)\n",
            "  Downloading bids_validator-1.12.0-py2.py3-none-any.whl (21 kB)\n",
            "Collecting num2words>=0.5.5 (from pybids>=0.15.2->templateflow)\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from pybids>=0.15.2->templateflow) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->templateflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->templateflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->templateflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->templateflow) (2023.7.22)\n",
            "Collecting astor>=0.8 (from formulaic<0.6,>=0.2.4->pybids>=0.15.2->templateflow)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic<0.6,>=0.2.4->pybids>=0.15.2->templateflow)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic<0.6,>=0.2.4->pybids>=0.15.2->templateflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic<0.6,>=0.2.4->pybids>=0.15.2->templateflow) (1.14.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from nibabel>=3.0->pybids>=0.15.2->templateflow) (23.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel>=3.0->pybids>=0.15.2->templateflow) (67.7.2)\n",
            "Collecting docopt>=0.6.2 (from num2words>=0.5.5->pybids>=0.15.2->templateflow)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.2->pybids>=0.15.2->templateflow) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.2->pybids>=0.15.2->templateflow) (2023.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.16->pybids>=0.15.2->templateflow) (2.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.25.2->pybids>=0.15.2->templateflow) (1.16.0)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=09e18f1d2523c46c017c1f05741906fcb7c295ad70f16c956987b522eff10378\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, bids-validator, num2words, interface-meta, astor, formulaic, pybids, templateflow\n",
            "Successfully installed astor-0.8.1 bids-validator-1.12.0 docopt-0.6.2 formulaic-0.5.2 interface-meta-1.3.0 num2words-0.5.12 pybids-0.16.3 templateflow-23.0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://templateflow.s3.amazonaws.com/tpl-MNI152NLin2009cAsym/tpl-MNI152NLin2009cAsym_res-01_T1w.nii.gz\n",
            "100%|██████████| 13.7M/13.7M [00:00<00:00, 18.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the template, the pipelines we can actually load the data. But first let's define the relevant directories."
      ],
      "metadata": {
        "id": "AxILefvkvf9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install ants\n",
        "#!pip install nipype\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import urllib.request\n",
        "from nipype.interfaces.ants import ApplyTransforms\n",
        "\n",
        "# Define paths\n",
        "data_directory = \"./data/participant_info\"\n",
        "transformed_directory = './drive/MyDrive/MasterThesisData/transformed'\n",
        "temp_directory = './temp_data'  # Temporary directory\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(transformed_directory, exist_ok=True)\n",
        "os.makedirs(temp_directory, exist_ok=True)\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file_path = os.path.join(data_directory, \"participant_info.csv\")\n",
        "data = pd.read_csv(csv_file_path)\n",
        "\n",
        "failed_download = []"
      ],
      "metadata": {
        "id": "O_YvCuNcvgh5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's attempt this with one participant and see if it works."
      ],
      "metadata": {
        "id": "keieS0zAuZ8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_id = \"50003\"\n",
        "\n",
        "try:\n",
        "    # Construct the MRI data file name\n",
        "    mri_data_file = f\"{file_id}_anat_thickness.nii.gz\"\n",
        "    mri_data_path = os.path.join(temp_directory, mri_data_file)\n",
        "\n",
        "    # Download the MRI data file to the temporary directory\n",
        "    mri_url = f\"https://fcp-indi.s3.amazonaws.com/data/Projects/ABIDE/Outputs/mindboggle_swf/mindboggle/ants_subjects/sub-00{sub_id}/antsCorticalThickness.nii.gz\"\n",
        "    urllib.request.urlretrieve(mri_url, mri_data_path)\n",
        "\n",
        "    # Construct the transformation template file name\n",
        "    template_file_name = f'sub-00{sub_id}_from-T1w_to-MNI152NLin2009cAsym_mode-image_xfm.h5'\n",
        "    template_destination = os.path.join(temp_directory, template_file_name)\n",
        "\n",
        "    # Download the transformation template\n",
        "    template_url = f\"https://fcp-indi.s3.amazonaws.com/data/Projects/ABIDE/Outputs/fmriprep/fmriprep/sub-00{sub_id}/anat/{template_file_name}\"\n",
        "    urllib.request.urlretrieve(template_url, template_destination)\n",
        "\n",
        "    # Define paths for the transformed output\n",
        "    transformed_output_path = os.path.join(transformed_directory, f\"sub-00{sub_id}_space-MNI152NLin2009cAsym_desc-corticalthickness.nii.gz\")\n",
        "\n",
        "    from nipype import config, logging\n",
        "    from nipype.interfaces.base import CommandLine\n",
        "\n",
        "    # Set the execution plugin to CommandLine\n",
        "    config.enable_debug_mode()\n",
        "    config.enable_provenance()\n",
        "    logging.update_logging(config)\n",
        "\n",
        "    # Define the antsApplyTransforms interface\n",
        "    at = ApplyTransforms()\n",
        "    at.inputs.input_image = mri_data_path\n",
        "    at.inputs.reference_image = mni152_path\n",
        "    at.inputs.transforms = template_destination\n",
        "    at.inputs.output_image = transformed_output_path\n",
        "\n",
        "    # Execute the antsApplyTransforms command\n",
        "    cmdline = at.cmdline\n",
        "    cl = CommandLine(command=cmdline)\n",
        "    cl.run()\n",
        "\n",
        "    # Clean up: Delete the input data and template\n",
        "    !rm \"./temp_data/\"\n",
        "\n",
        "    print(f\"Transformed and saved for FILE_ID: {file_id}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed for FILE_ID: {file_id}. Error: {e}\")\n"
      ],
      "metadata": {
        "id": "VVu3Eo5Yu2lA",
        "outputId": "175890ef-1018-4589-ce2a-7c83e55a4400",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230811-22:38:05,222 nipype.interface DEBUG:\n",
            "\t default_value_0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:nipype.interface:default_value_0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230811-22:38:05,248 nipype.interface DEBUG:\n",
            "\t float_False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:nipype.interface:float_False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230811-22:38:05,271 nipype.interface DEBUG:\n",
            "\t input_image_temp_data/Pitt_0050014_anat_thickness.nii.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:nipype.interface:input_image_temp_data/Pitt_0050014_anat_thickness.nii.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230811-22:38:05,292 nipype.interface DEBUG:\n",
            "\t reference_image_/root/.cache/templateflow/tpl-MNI152NLin2009cAsym/tpl-MNI152NLin2009cAsym_res-01_T1w.nii.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:nipype.interface:reference_image_/root/.cache/templateflow/tpl-MNI152NLin2009cAsym/tpl-MNI152NLin2009cAsym_res-01_T1w.nii.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove './temp_data': Is a directory\n",
            "Transformed and saved for FILE_ID: Pitt_0050014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Now onto all the other participants."
      ],
      "metadata": {
        "id": "cPcMSHMquTvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in data.iterrows():\n",
        "    file_id = row['FILE_ID']\n",
        "    sub_id = row['SUB_ID']\n",
        "\n",
        "    try:\n",
        "        # Construct the MRI data file name\n",
        "        mri_data_file = f\"{file_id}_anat_thickness.nii.gz\"\n",
        "        mri_data_path = os.path.join(temp_directory, mri_data_file)\n",
        "\n",
        "        # Download the MRI data file to the temporary directory\n",
        "        mri_url = f\"https://fcp-indi.s3.amazonaws.com/data/Projects/ABIDE/Outputs/mindboggle_swf/mindboggle/ants_subjects/sub-00{sub_id}/antsCorticalThickness.nii.gz\"\n",
        "        urllib.request.urlretrieve(mri_url, mri_data_path)\n",
        "\n",
        "        # Construct the transformation template file name\n",
        "        template_file_name = f'sub-00{sub_id}_from-T1w_to-MNI152NLin2009cAsym_mode-image_xfm.h5'\n",
        "        template_destination = os.path.join(temp_directory, template_file_name)\n",
        "\n",
        "        # Download the transformation template\n",
        "        template_url = f\"https://fcp-indi.s3.amazonaws.com/data/Projects/ABIDE/Outputs/fmriprep/fmriprep/sub-00{sub_id}/anat/{template_file_name}\"\n",
        "        urllib.request.urlretrieve(template_url, template_destination)\n",
        "\n",
        "        # Define paths for the transformed output\n",
        "        transformed_output_path = os.path.join(transformed_directory, f\"sub-00{sub_id}_space-MNI152NLin2009cAsym_desc-corticalthickness.nii.gz\")\n",
        "\n",
        "        from nipype import config, logging\n",
        "        from nipype.interfaces.base import CommandLine\n",
        "\n",
        "        # Set the execution plugin to CommandLine\n",
        "        config.enable_debug_mode()\n",
        "        config.enable_provenance()\n",
        "        logging.update_logging(config)\n",
        "\n",
        "        # Define the antsApplyTransforms interface\n",
        "        at = ApplyTransforms()\n",
        "        at.inputs.input_image = mri_data_path\n",
        "        at.inputs.reference_image = mni152_path\n",
        "        at.inputs.transforms = template_destination\n",
        "        at.inputs.output_image = transformed_output_path\n",
        "\n",
        "        # Execute the antsApplyTransforms command\n",
        "        cmdline = at.cmdline\n",
        "        cl = CommandLine(command=cmdline)\n",
        "        cl.run()\n",
        "\n",
        "        # Clean up: Delete the input data and template\n",
        "        !rm \"./temp_data\"\n",
        "\n",
        "        print(f\"Transformed and saved for FILE_ID: {file_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed for FILE_ID: {file_id}. Error: {e}\")\n",
        "\n",
        "print(\"Processing completed.\")\n"
      ],
      "metadata": {
        "id": "OD23ChgQuY4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I need to actually apply the atlas. In this step I am loading the atlas and the corresponding labels."
      ],
      "metadata": {
        "id": "f6u2j6ezEE4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the atlas\n",
        "from nilearn.datasets import fetch_atlas_destrieux_2009\n",
        "from nilearn.image import load_img\n",
        "\n",
        "atlas_destrieux_dataset = fetch_atlas_destrieux_2009(lateralized = True)\n",
        "atlas_destrieux = load_img(atlas_destrieux_dataset.maps)\n",
        "\n",
        "# From this, we can also export the labels that we can use for later visualization.\n",
        "labels = atlas_destrieux_dataset.labels\n",
        "\n"
      ],
      "metadata": {
        "id": "DMXFKprcENs5",
        "outputId": "077f35e4-396c-4227-c09a-ee2a9099c179",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset created in /root/nilearn_data/destrieux_2009\n",
            "\n",
            "Downloading data from https://www.nitrc.org/frs/download.php/11942/destrieux2009.tgz ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " ...done. (1 seconds, 0 min)\n",
            "Extracting data from /root/nilearn_data/destrieux_2009/2a2e5a5707983d509d9319c692c867ab/destrieux2009.tgz..... done.\n",
            "/usr/local/lib/python3.10/dist-packages/nilearn/datasets/atlas.py:355: UserWarning: `legacy_format` will default to `False` in release 0.11. Dataset fetchers will then return pandas dataframes by default instead of recarrays.\n",
            "  warnings.warn(_LEGACY_FORMAT_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use the niftilabelmasker to actually apply the mask. Of course I should use a loop here later!"
      ],
      "metadata": {
        "id": "S2rZ7uH4HSLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nilearn.maskers import NiftiLabelsMasker\n",
        "\n",
        "masker_destrieux = NiftiLabelsMasker(labels_img=atlas_destrieux)\n",
        "\n",
        "thickness_data_dict = {}\n",
        "\n",
        "for filename in os.listdir(transformed_directory):\n",
        "    if filename.endswith(\"_desc-corticalthickness.nii.gz\"):\n",
        "        sub_id = filename.split(\"_\")[0].replace(\"sub-\", \"\")\n",
        "        thickness_filename = f\"thickness_{sub_id}\"\n",
        "\n",
        "        thickness_data = masker_destrieux.fit_transform(os.path.join(transformed_directory, filename))\n",
        "\n",
        "        thickness_data_dict[sub_id] = {\n",
        "            \"sub_id\": sub_id,\n",
        "            \"data\": thickness_data\n",
        "        }\n",
        "\n",
        "        print(f\"Processed for sub_id: {sub_id}\")\n",
        "        print(globals()[thickness_filename])\n"
      ],
      "metadata": {
        "id": "3-lDYYfox65g",
        "outputId": "af65c0de-4a7c-4fe3-d783-729b74238fbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed for sub_id: 0050003\n",
            "[[1.24944175 2.34536168 0.606836   1.71352572 1.52545434 3.10993156\n",
            "  2.04731399 2.10898342 2.92694597 1.52362547 1.25300568 1.78683702\n",
            "  1.36531509 1.33183801 1.72617632 1.79656362 2.45268168 3.06566965\n",
            "  1.91948329 1.06945715 3.09177802 1.51964456 2.92252828 1.61312081\n",
            "  1.33889796 1.75681046 0.82938553 0.79790724 1.27660216 1.80350527\n",
            "  2.29007522 0.92637947 1.33526913 1.97557859 1.29265578 1.15743216\n",
            "  2.56120038 2.35720902 1.17724812 1.49941028 1.3729409  1.41701773\n",
            "  2.84152904 1.25315129 0.46842935 1.03995316 1.60925862 0.85085033\n",
            "  1.84187833 2.91596348 2.02335056 1.37771389 0.84202682 1.40546209\n",
            "  0.86230999 0.8225973  0.99084249 1.25135441 2.01259242 3.08581815\n",
            "  2.71363081 0.60009495 0.88043531 1.34770937 1.15595373 0.83335493\n",
            "  0.80230146 1.0742383  0.84154005 3.80625478 2.01734593 1.77281437\n",
            "  1.52858828 0.67798723 1.59052564 2.10532643 0.65951737 1.84252687\n",
            "  1.8731977  3.08255791 2.57705834 2.65432732 3.29420721 1.85932626\n",
            "  1.59324833 1.62434517 2.00253108 1.48929939 2.11547331 1.88203491\n",
            "  2.99583286 3.50206556 1.76242735 1.30028175 2.86001846 1.4645331\n",
            "  2.90693984 1.60345748 1.42553496 1.89063294 0.6637651  0.705717\n",
            "  1.05332134 1.9460822  2.74310166 0.94061779 1.54846976 1.69223849\n",
            "  0.97717156 0.58586031 2.83781352 2.73770981 1.36209438 0.71621789\n",
            "  1.02328903 1.78896477 2.82955619 1.35781833 0.37849333 1.09557431\n",
            "  2.22725458 0.87094703 1.8186979  2.36792727 2.17386297 1.19262718\n",
            "  1.21617019 1.7952098  1.36532848 0.96187317 1.19106197 1.24348792\n",
            "  1.53188043 3.10959993 2.29012877 0.73042764 1.13302745 1.48995839\n",
            "  1.39482021 0.95580911 1.01340645 0.78417804 0.92455626 2.66533118\n",
            "  2.79238291 1.73645045 1.42436385 0.54438221]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a next step I will use this data to create dictionaries containing all the data, as well as the labels that will be used for training."
      ],
      "metadata": {
        "id": "nzGFEpf_Sm7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the participant information from the CSV file\n",
        "csv_file = \"./data/participant_info/participant_info.csv\"\n",
        "participant_info_df = pd.read_csv(csv_file)\n",
        "\n",
        "# Create dictionaries to store data and participant information for autism and non-autism participants\n",
        "data_info_dict_autism = {}\n",
        "data_info_dict_no_autism = {}\n",
        "\n",
        "# Loop through the thickness data dictionary\n",
        "for sub_id, thickness_data_entry in thickness_data_dict.items():\n",
        "    # Find the participant's information based on SUB_ID\n",
        "    participant_row = participant_info_df.loc[participant_info_df['SUB_ID'] == int(sub_id)]\n",
        "    if not participant_row.empty:\n",
        "        # Extract age, gender, and dx_group from the participant's information\n",
        "        age = participant_row['AGE_AT_SCAN'].values[0]\n",
        "        gender = participant_row['SEX'].values[0] - 1\n",
        "        dx_group = participant_row['DX_GROUP'].values[0]\n",
        "\n",
        "        # Create a data entry for the sub ID\n",
        "        data_entry = {\n",
        "            \"data\": thickness_data_entry['data'],\n",
        "            \"age\": age,\n",
        "            \"gender\": gender\n",
        "        }\n",
        "\n",
        "        # Store the data and participant information in the appropriate dictionary based on DX_GROUP\n",
        "        if dx_group == 1:\n",
        "            data_info_dict_autism[sub_id] = data_entry\n",
        "        elif dx_group == 2:\n",
        "            data_info_dict_no_autism[sub_id] = data_entry\n",
        "    else:\n",
        "        print(f\"No participant information found for sub_id: {sub_id}\")\n",
        "\n",
        "total_length = len(data_info_dict_autism) + len(data_info_dict_no_autism)\n",
        "print(f\"Total length of combined dictionaries: {total_length}\")"
      ],
      "metadata": {
        "id": "vmBh6F8mP8S5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I also see how many files the dictionaries contain. Does this match with the overall number of files?"
      ],
      "metadata": {
        "id": "YY_z46AT5akV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! cd ./data & du -a | cut -d/ -f2 | sort | uniq -c | sort -nr"
      ],
      "metadata": {
        "id": "K72_ISsv5WEG",
        "outputId": "37fb541d-c674-4174-8f7b-ff621bb536f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     54 drive\n",
            "     18 .config\n",
            "      7 sample_data\n",
            "      3 data\n",
            "      1 temp_data\n",
            "      1 googlecolab_setup.sh\n",
            "      1 cvmfs-release-latest_all.deb\n",
            "      1 apptainer-suid_1.2.0_amd64.deb\n",
            "      1 apptainer_1.2.0_amd64.deb\n",
            "      1 96938\t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_info_dict_no_autism = data_info_dict_autism   #Keeping this here when only working with one participant, so both dictionaries have an entry."
      ],
      "metadata": {
        "id": "bhQvps9SiU8d"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check if this all worked:"
      ],
      "metadata": {
        "id": "AESZ6k9heLiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate overall statistics for the autism category\n",
        "autism_data_lengths = [len(info[\"data\"]) for info in data_info_dict_autism.values()]\n",
        "total_autism_samples = len(autism_data_lengths)\n",
        "average_autism_data_length = sum(autism_data_lengths) / total_autism_samples\n",
        "min_autism_data_length = min(autism_data_lengths)\n",
        "max_autism_data_length = max(autism_data_lengths)\n",
        "std_autism_data_length = np.std(autism_data_lengths)\n",
        "autism_ages = [info[\"age\"] for info in data_info_dict_autism.values()]\n",
        "average_autism_age = sum(autism_ages) / total_autism_samples\n",
        "min_autism_age = min(autism_ages)\n",
        "max_autism_age = max(autism_ages)\n",
        "std_autism_age = np.std(autism_ages)\n",
        "autism_genders = [info[\"gender\"] for info in data_info_dict_autism.values()]\n",
        "# Calculate gender counts for the autism category\n",
        "autism_male_count = autism_genders.count(0)\n",
        "autism_female_count = autism_genders.count(1)\n",
        "\n",
        "# Calculate overall statistics for the non-autism category\n",
        "non_autism_data_lengths = [len(info[\"data\"]) for info in data_info_dict_no_autism.values()]\n",
        "total_non_autism_samples = len(non_autism_data_lengths)\n",
        "average_non_autism_data_length = sum(non_autism_data_lengths) / total_non_autism_samples\n",
        "min_non_autism_data_length = min(non_autism_data_lengths)\n",
        "max_non_autism_data_length = max(non_autism_data_lengths)\n",
        "std_non_autism_data_length = np.std(non_autism_data_lengths)\n",
        "non_autism_ages = [info[\"age\"] for info in data_info_dict_no_autism.values()]\n",
        "average_non_autism_age = sum(non_autism_ages) / total_non_autism_samples\n",
        "min_non_autism_age = min(non_autism_ages)\n",
        "max_non_autism_age = max(non_autism_ages)\n",
        "std_non_autism_age = np.std(non_autism_ages)\n",
        "non_autism_genders = [info[\"gender\"] for info in data_info_dict_no_autism.values()]\n",
        "# Calculate gender counts for the non-autism category\n",
        "non_autism_male_count = non_autism_genders.count(0)\n",
        "non_autism_female_count = non_autism_genders.count(1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Print the statistics for the autism category\n",
        "print(\"Autism Data Statistics:\")\n",
        "print(\"Total Samples:\", total_autism_samples)\n",
        "print(\"Average Data Length:\", average_autism_data_length)\n",
        "print(\"Minimum Data Length:\", min_autism_data_length)\n",
        "print(\"Maximum Data Length:\", max_autism_data_length)\n",
        "print(\"Standard Deviation of Data Length:\", std_autism_data_length)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Autism Age Statistics:\")\n",
        "print(\"Average Age:\", average_autism_age)\n",
        "print(\"Minimum Age:\", min_autism_age)\n",
        "print(\"Maximum Age:\", max_autism_age)\n",
        "print(\"Standard Deviation of Age:\", std_autism_age)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Autism Gender Counts:\")\n",
        "print(\"Male Count:\", autism_male_count)\n",
        "print(\"Female Count:\", autism_female_count)\n",
        "print(\"\")\n",
        "\n",
        "# Print the statistics for the non-autism category\n",
        "print(\"Non-Autism Data Statistics:\")\n",
        "print(\"Total Samples:\", total_non_autism_samples)\n",
        "print(\"Average Data Length:\", average_non_autism_data_length)\n",
        "print(\"Minimum Data Length:\", min_non_autism_data_length)\n",
        "print(\"Maximum Data Length:\", max_non_autism_data_length)\n",
        "print(\"Standard Deviation of Data Length:\", std_non_autism_data_length)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Non-Autism Age Statistics:\")\n",
        "print(\"Average Age:\", average_non_autism_age)\n",
        "print(\"Minimum Age:\", min_non_autism_age)\n",
        "print(\"Maximum Age:\", max_non_autism_age)\n",
        "print(\"Standard Deviation of Age:\", std_non_autism_age)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Non-Autism Gender Counts:\")\n",
        "print(\"Male Count:\", non_autism_male_count)\n",
        "print(\"Female Count:\", non_autism_female_count)\n"
      ],
      "metadata": {
        "id": "z8L4eCdBeNfa",
        "outputId": "1bfd0f1c-9f01-47b1-cce0-f1dc044a6e41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autism Data Statistics:\n",
            "Total Samples: 1\n",
            "Average Data Length: 1.0\n",
            "Minimum Data Length: 1\n",
            "Maximum Data Length: 1\n",
            "Standard Deviation of Data Length: 0.0\n",
            "\n",
            "Autism Age Statistics:\n",
            "Average Age: 24.45\n",
            "Minimum Age: 24.45\n",
            "Maximum Age: 24.45\n",
            "Standard Deviation of Age: 0.0\n",
            "\n",
            "Autism Gender Counts:\n",
            "Male Count: 1\n",
            "Female Count: 0\n",
            "\n",
            "Non-Autism Data Statistics:\n",
            "Total Samples: 1\n",
            "Average Data Length: 1.0\n",
            "Minimum Data Length: 1\n",
            "Maximum Data Length: 1\n",
            "Standard Deviation of Data Length: 0.0\n",
            "\n",
            "Non-Autism Age Statistics:\n",
            "Average Age: 24.45\n",
            "Minimum Age: 24.45\n",
            "Maximum Age: 24.45\n",
            "Standard Deviation of Age: 0.0\n",
            "\n",
            "Non-Autism Gender Counts:\n",
            "Male Count: 1\n",
            "Female Count: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I need to create a dataloader."
      ],
      "metadata": {
        "id": "B3Aho7KLeWWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedDataset(Dataset):\n",
        "    def __init__(self, autism_data_info, no_autism_data_info):\n",
        "        self.autism_data_info = autism_data_info\n",
        "        self.no_autism_data_info = no_autism_data_info\n",
        "        self.autism_file_ids = list(self.autism_data_info.keys())\n",
        "        self.no_autism_file_ids = list(self.no_autism_data_info.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(len(self.autism_file_ids), len(self.no_autism_file_ids))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        autism_index = index % len(self.autism_file_ids)\n",
        "        no_autism_index = index % len(self.no_autism_file_ids)\n",
        "\n",
        "        autism_file_id = self.autism_file_ids[autism_index]\n",
        "        no_autism_file_id = self.no_autism_file_ids[no_autism_index]\n",
        "\n",
        "        autism_data = torch.tensor(self.autism_data_info[autism_file_id][\"data\"], dtype=torch.float32)\n",
        "        autism_age = torch.tensor(self.autism_data_info[autism_file_id][\"age\"], dtype=torch.float32)\n",
        "        autism_gender = torch.tensor(self.autism_data_info[autism_file_id][\"gender\"], dtype=torch.float32)\n",
        "\n",
        "        no_autism_data = torch.tensor(self.no_autism_data_info[no_autism_file_id][\"data\"], dtype=torch.float32)\n",
        "        no_autism_age = torch.tensor(self.no_autism_data_info[no_autism_file_id][\"age\"], dtype=torch.float32)\n",
        "        no_autism_gender = torch.tensor(self.no_autism_data_info[no_autism_file_id][\"gender\"], dtype=torch.float32)\n",
        "\n",
        "        return (autism_data, autism_age, autism_gender), (no_autism_data, no_autism_age, no_autism_gender)\n",
        "\n",
        "# Create the combined dataset\n",
        "combined_dataset = CombinedDataset(data_info_dict_autism, data_info_dict_no_autism)\n",
        "\n",
        "# Create the dataloader\n",
        "batch_size = 64\n",
        "shuffle = True\n",
        "combined_dataloader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=shuffle)\n"
      ],
      "metadata": {
        "id": "bC421VYyebzD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I have one complete data-set. Of course it's adviseable to split the data into data for training and data for validation. Do note that the code below currently failed as I only have one participant preprocessed."
      ],
      "metadata": {
        "id": "iH3LuD-hwDQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import Subset\n",
        "\n",
        "# Create the combined dataset, this was already done\n",
        "#combined_dataset = CombinedDataset(data_info_dict_autism, data_info_dict_no_autism)\n",
        "\n",
        "# Calculate the size of training and validation sets\n",
        "total_size = len(combined_dataset)\n",
        "train_size = int(0.8 * total_size)  # 80% for training, adjust as needed\n",
        "val_size = total_size - train_size\n",
        "\n",
        "# Split indices for training and validation\n",
        "train_indices = list(range(train_size))\n",
        "val_indices = list(range(train_size, total_size))\n",
        "\n",
        "# Create Subset datasets\n",
        "train_dataset = Subset(combined_dataset, train_indices)\n",
        "val_dataset = Subset(combined_dataset, val_indices)\n",
        "\n",
        "# Create dataloaders for training and validation\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle the validation set"
      ],
      "metadata": {
        "id": "GhBA3y1WwCp0",
        "outputId": "bd5155d6-0528-4923-c770-88de80f290c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-f46d3b7c6896>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Create dataloaders for training and validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# No need to shuffle the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[1;32m    108\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model specifications\n",
        "\n",
        "In the following I am specifiying the model. I am roughly orienting myself around a paper from Anglinkas, Hartshorne & Anzellotti (2022).\n",
        "\n",
        "### Defining utility functions\n",
        "\n",
        "Firstly, I am defining the loss function.\n",
        "The loss will be computed as the sum of the BCE-Loss, as well as the KL-divergence terms.\n",
        "\n",
        "* MSE loss: Incoming\n",
        "\n",
        "* Cross Entropy: Incoming\n",
        "\n",
        "* Kullback-Leibler divergence (Kullback & Leibler, 1951) This is a measure for the difference between two distributions. I.e. \"how much do they diverge\" from each other, how much are they different to each other. The introduction of this term into the final loss leads my model to optimize not only if the precited categories are correct and so on, but also how high the difference between the prior distribution and teh latent variables are. The prior distribution in my case is an isotropic gaussian.\n",
        "  * Why is this desirable? The latent variables and the sampling process should be somewhat controlled. This divergence regulates this.\n",
        "\n",
        "\n",
        "I have also attempted to regulate that a loss is only completed with the KL divergence from the second encoder if that encoder was used."
      ],
      "metadata": {
        "id": "NV2DSYrow9-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_loss(MSE, CE, MSE_age, z_mu, z_logvar, s_mu, s_logvar):\n",
        "    \"\"\"\n",
        "    This function will add the reconstruction loss (BCELoss) and the KL-Divergence.\n",
        "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    :param bce_loss: reconstruction loss\n",
        "    :param z_mu: mean from the latent vector of encoder_z\n",
        "    :param z_logvar: log variance from the latent vector of encoder_z\n",
        "    :param s_mu: mean from the latent vector of encoder_s (optional)\n",
        "    :param s_logvar: log variance from the latent vector of encoder_s (optional)\n",
        "    \"\"\"\n",
        "    mse_loss = MSE\n",
        "    mse_age = MSE_age\n",
        "    cross_entropy = CE\n",
        "    KLD_z = -0.5 * torch.sum(1 + z_logvar - z_mu.pow(2) - z_logvar.exp())\n",
        "    if s_mu is not None and s_logvar is not None:\n",
        "        KLD_s = -0.5 * torch.sum(1 + s_logvar - s_mu.pow(2) - s_logvar.exp())\n",
        "        return mse_loss + KLD_z + KLD_s + cross_entropy + mse_age\n",
        "    else:\n",
        "        return mse_loss + KLD_z + cross_entropy + mse_age\n"
      ],
      "metadata": {
        "id": "5WsDILlQlYya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the training loop. This model is supposed to achieve multiple things:\n",
        "\n",
        "* Train the cVAE using the MSE loss.\n",
        "* Incoming.\n"
      ],
      "metadata": {
        "id": "cV4DbbobmW5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, dataloader, dataset, device, optimizer, criterion, criterion_classifier):\n",
        "    model.train()\n",
        "    running_loss_autism = 0.0\n",
        "    running_loss_no_autism = 0.0\n",
        "    running_age_loss_autism = 0.0\n",
        "    running_gender_loss_autism = 0.0\n",
        "    running_age_loss_no_autism = 0.0\n",
        "    running_gender_loss_no_autism = 0.0\n",
        "    counter = 0\n",
        "\n",
        "    total_batches = len(dataset) // dataloader.batch_size\n",
        "\n",
        "    for i, ((autism_data, autism_age, autism_gender), (no_autism_data, no_autism_age, no_autism_gender)) in tqdm(enumerate(dataloader), total=total_batches):\n",
        "        autism_data = autism_data.to(device)\n",
        "        no_autism_data = no_autism_data.to(device)\n",
        "\n",
        "        autism_age = autism_age.to(device)\n",
        "        autism_gender = autism_gender.to(device)\n",
        "\n",
        "        no_autism_age = no_autism_age.to(device)\n",
        "        no_autism_gender = no_autism_gender.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the model outputs\n",
        "        z_mean, z_log_var, s_mean, s_log_var, z_mean_no_autism, z_log_var_no_autism, reconstructed_data_autism, reconstructed_data_no_autism, class_autism_age, class_autism_gender, class_no_autism_age, class_no_autism_gender = model(autism_data, no_autism_data)\n",
        "\n",
        "        # Calculate classifier losses for age and gender predictions\n",
        "        age_loss_autism = criterion(class_autism_age, autism_age.unsqueeze(1))\n",
        "        gender_loss_autism = criterion_classifier(class_autism_gender, autism_gender.unsqueeze(1))\n",
        "\n",
        "        age_loss_no_autism = criterion(class_no_autism_age, no_autism_age.unsqueeze(1))\n",
        "        gender_loss_no_autism = criterion_classifier(class_no_autism_gender, no_autism_gender.unsqueeze(1))\n",
        "\n",
        "        # Section for the autism images\n",
        "        bce_loss_autism = criterion(reconstructed_data_autism, autism_data)\n",
        "        loss_autism = final_loss(bce_loss_autism,  gender_loss_autism, age_loss_autism, z_mean, z_log_var, s_mean, s_log_var)\n",
        "        running_loss_autism += loss_autism.item()\n",
        "        running_age_loss_autism += age_loss_autism.item()\n",
        "        running_gender_loss_autism += gender_loss_autism.item()\n",
        "\n",
        "        # Section for the no_autism images\n",
        "        bce_loss_no_autism = criterion(reconstructed_data_no_autism, no_autism_data)\n",
        "        s_mean_no_autism, s_log_var_no_autism = None, None\n",
        "        loss_no_autism = final_loss(bce_loss_no_autism,  gender_loss_no_autism, age_loss_no_autism, z_mean_no_autism, z_log_var_no_autism, s_mean_no_autism, s_log_var_no_autism)\n",
        "        running_loss_no_autism += loss_no_autism.item()\n",
        "        running_age_loss_no_autism += age_loss_no_autism.item()\n",
        "        running_gender_loss_no_autism += gender_loss_no_autism.item()\n",
        "\n",
        "        # Total loss\n",
        "        loss_no_autism.backward()\n",
        "        loss_autism.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        counter += len(autism_data) + len(no_autism_data)\n",
        "\n",
        "    train_loss_autism = running_loss_autism / counter\n",
        "    train_loss_no_autism = running_loss_no_autism / counter\n",
        "    train_age_loss_autism = running_age_loss_autism / counter\n",
        "    train_gender_loss_autism = running_gender_loss_autism / counter\n",
        "    train_age_loss_no_autism = running_age_loss_no_autism / counter\n",
        "    train_gender_loss_no_autism = running_gender_loss_no_autism / counter\n",
        "\n",
        "    return train_loss_autism, train_loss_no_autism, train_age_loss_autism, train_gender_loss_autism, train_age_loss_no_autism, train_gender_loss_no_autism\n"
      ],
      "metadata": {
        "id": "6_qe6uSIfoHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And of course the corresponding validation function\n"
      ],
      "metadata": {
        "id": "WkYnidFSwwLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataloader, dataset, device, criterion, criterion_classifier):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    running_loss_autism = 0.0\n",
        "    running_loss_no_autism = 0.0\n",
        "    running_age_loss_autism = 0.0\n",
        "    running_gender_loss_autism = 0.0\n",
        "    running_age_loss_no_autism = 0.0\n",
        "    running_gender_loss_no_autism = 0.0\n",
        "    counter = 0\n",
        "\n",
        "    total_batches = len(dataset) // dataloader.batch_size\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation during validation\n",
        "        for i, ((autism_data, autism_age, autism_gender), (no_autism_data, no_autism_age, no_autism_gender)) in tqdm(enumerate(dataloader), total=total_batches):\n",
        "            autism_data = autism_data.to(device)\n",
        "            no_autism_data = no_autism_data.to(device)\n",
        "\n",
        "            autism_age = autism_age.to(device)\n",
        "            autism_gender = autism_gender.to(device)\n",
        "\n",
        "            no_autism_age = no_autism_age.to(device)\n",
        "            no_autism_gender = no_autism_gender.to(device)\n",
        "\n",
        "            # Get the model outputs\n",
        "            z_mean, z_log_var, s_mean, s_log_var, z_mean_no_autism, z_log_var_no_autism, reconstructed_data_autism, reconstructed_data_no_autism, class_autism_age, class_autism_gender, class_no_autism_age, class_no_autism_gender = model(autism_data, no_autism_data)\n",
        "\n",
        "            # Calculate classifier losses for age and gender predictions\n",
        "            age_loss_autism = criterion(class_autism_age, autism_age.unsqueeze(1))\n",
        "            gender_loss_autism = criterion_classifier(class_autism_gender, autism_gender.unsqueeze(1))\n",
        "\n",
        "            age_loss_no_autism = criterion(class_no_autism_age, no_autism_age.unsqueeze(1))\n",
        "            gender_loss_no_autism = criterion_classifier(class_no_autism_gender, no_autism_gender.unsqueeze(1))\n",
        "\n",
        "            # Section for the autism images\n",
        "            bce_loss_autism = criterion(reconstructed_data_autism, autism_data)\n",
        "            loss_autism = final_loss(bce_loss_autism, gender_loss_autism, age_loss_autism, z_mean, z_log_var, s_mean, s_log_var)\n",
        "            running_loss_autism += loss_autism.item()\n",
        "            running_age_loss_autism += age_loss_autism.item()\n",
        "            running_gender_loss_autism += gender_loss_autism.item()\n",
        "\n",
        "            # Section for the no_autism images\n",
        "            bce_loss_no_autism = criterion(reconstructed_data_no_autism, no_autism_data)\n",
        "            s_mean_no_autism, s_log_var_no_autism = None, None\n",
        "            loss_no_autism = final_loss(bce_loss_no_autism, gender_loss_no_autism, age_loss_no_autism, z_mean_no_autism, z_log_var_no_autism, s_mean_no_autism, s_log_var_no_autism)\n",
        "            running_loss_no_autism += loss_no_autism.item()\n",
        "            running_age_loss_no_autism += age_loss_no_autism.item()\n",
        "            running_gender_loss_no_autism += gender_loss_no_autism.item()\n",
        "\n",
        "            counter += len(autism_data) + len(no_autism_data)\n",
        "\n",
        "    val_loss_autism = running_loss_autism / counter\n",
        "    val_loss_no_autism = running_loss_no_autism / counter\n",
        "    val_age_loss_autism = running_age_loss_autism / counter\n",
        "    val_gender_loss_autism = running_gender_loss_autism / counter\n",
        "    val_age_loss_no_autism = running_age_loss_no_autism / counter\n",
        "    val_gender_loss_no_autism = running_gender_loss_no_autism / counter\n",
        "\n",
        "    return (\n",
        "        val_loss_autism, val_loss_no_autism,\n",
        "        val_age_loss_autism, val_gender_loss_autism,\n",
        "        val_age_loss_no_autism, val_gender_loss_no_autism\n",
        "    )\n"
      ],
      "metadata": {
        "id": "r983E3vEwy-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model specification\n",
        "\n",
        "These values still need to be adapted for the current model."
      ],
      "metadata": {
        "id": "rtNUqWlK6YfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dimension = 97 # The numer of features\n",
        "indermediate_dim = 128\n",
        "latent_dim = 4 # latent dimension for sampling\n",
        "\n",
        "lr = 0.01"
      ],
      "metadata": {
        "id": "1NXSPtSc6biA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next I want to define the contrastive variational autoencoder. While doing so, I am defining seperate encoders, to make it easier to later introduce other encoders. I am orienting myself on an cVAE I have written in the past.\n",
        "\n",
        "As the paper from Aglinskas, Hartshorne and Anzellotti (2022) I mentioned, the network will have only a few layers.\n",
        "\n",
        "A few things I will probably have to change - I do not know how many channels the data will end up having. therefore I am using one, assuming it only has one."
      ],
      "metadata": {
        "id": "yzEoxSr08h-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderNS(nn.Module):\n",
        "    def __init__(self, input_dimension, latent_dim):\n",
        "        super(EncoderNS, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dimension, 64)\n",
        "        self.linear2 = nn.Linear(64, 32)\n",
        "        self.linear3 = nn.Linear(32, 4)\n",
        "        self.ns_fc_mean = nn.Linear(latent_dim, latent_dim)\n",
        "        self.ns_fc_log_var = nn.Linear(latent_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x, batch_size):\n",
        "        h = F.relu(self.linear1(x))\n",
        "        h = F.relu(self.linear2(h))\n",
        "        h = F.relu(self.linear3(h))\n",
        "        ns_mean = self.ns_fc_mean(h)\n",
        "        ns_log_var = self.ns_fc_log_var(h)\n",
        "        return ns_mean, ns_log_var\n",
        "\n",
        "\n",
        "class EncoderS(nn.Module):\n",
        "    def __init__(self, input_dimension, latent_dim):\n",
        "        super(EncoderS, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dimension, 64)\n",
        "        self.linear2 = nn.Linear(64, 32)\n",
        "        self.linear3 = nn.Linear(32, 4)\n",
        "        self.s_fc_mean = nn.Linear(latent_dim, latent_dim)\n",
        "        self.s_fc_log_var = nn.Linear(latent_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x, batch_size):\n",
        "        h = F.relu(self.linear1(x))\n",
        "        h = F.relu(self.linear2(h))\n",
        "        h = F.relu(self.linear3(h))\n",
        "        s_mean = self.s_fc_mean(h)\n",
        "        s_log_var = self.s_fc_log_var(h)\n",
        "        return s_mean, s_log_var\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dimension, latent_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.linear_decoder_1 = nn.Linear(latent_dim*2, 32)\n",
        "        self.linear_decoder_2 = nn.Linear(32,64)\n",
        "        self.linear_decoder_3 = nn.Linear(64, input_dimension)\n",
        "\n",
        "    def forward(self, zs, batch_size):\n",
        "        h_output = F.relu(self.linear_decoder_1(zs))\n",
        "        h_output = F.relu(self.linear_decoder_2(h_output))\n",
        "        output = F.relu(self.linear_decoder_3(h_output))\n",
        "        return output\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim*2, latent_dim // 2)\n",
        "        self.fc_age = nn.Linear(latent_dim // 2, 1)\n",
        "        self.fc_gender = nn.Linear(latent_dim // 2, 1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.fc1(z)\n",
        "        age_prediction = self.fc_age(x)\n",
        "        gender_prediction = torch.sigmoid(self.fc_gender(x))  # Apply sigmoid activation for binary gender prediction\n",
        "        return age_prediction, gender_prediction\n",
        "\n",
        "class cVAE(nn.Module):\n",
        "    def __init__(self, input_dimension, latent_dim):\n",
        "        super(cVAE, self).__init__()\n",
        "        self.encoder_z = EncoderNS(input_dimension, latent_dim)\n",
        "        self.encoder_s = EncoderS(input_dimension, latent_dim)\n",
        "        self.decoder = Decoder(input_dimension, latent_dim)\n",
        "        self.classifier = Classifier(latent_dim)\n",
        "\n",
        "    def reparameterize(self, mean, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        epsilon = torch.randn_like(std)\n",
        "        return mean + epsilon * std\n",
        "\n",
        "    def forward(self, autism, no_autism):\n",
        "        batch_size = autism.size(0)\n",
        "        z_mean, z_log_var = self.encoder_z(autism, batch_size)\n",
        "        z = self.reparameterize(z_mean, z_log_var)\n",
        "        s_mean, s_log_var = self.encoder_s(autism, batch_size)\n",
        "        s = self.reparameterize(s_mean, s_log_var)\n",
        "        zs = torch.cat([z, s], dim=1)\n",
        "\n",
        "        reconstructed_data_autism = self.decoder(zs, batch_size)\n",
        "\n",
        "        z_mean_no_autism, z_log_var_no_autism = self.encoder_z(no_autism, batch_size)\n",
        "        z_no_autism = self.reparameterize(z_mean_no_autism, z_log_var_no_autism)\n",
        "        z_empty = torch.zeros(z_no_autism.shape)\n",
        "        model_device = z_no_autism.device\n",
        "        z_empty = z_empty.to(model_device)\n",
        "        z_no_autism_0 = torch.cat([z_no_autism, z_empty], dim=1)\n",
        "\n",
        "        reconstructed_data_no_autism = self.decoder(z_no_autism_0, batch_size)\n",
        "\n",
        "        class_autism_age, class_autism_gender = self.classifier(zs)  # Assuming z is the latent variable after concatenating s and z\n",
        "        class_no_autism_age, class_no_autism_gender = self.classifier(z_no_autism_0)  # Using the version with 0s to have equal lengths of the latent vectors.\n",
        "\n",
        "        return z_mean, z_log_var, s_mean, s_log_var, z_mean_no_autism, z_log_var_no_autism, reconstructed_data_autism, reconstructed_data_no_autism, class_autism_age, class_autism_gender, class_no_autism_age, class_no_autism_gender"
      ],
      "metadata": {
        "id": "uwDJGACDj1kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally the training loop - note that I have yet to define the validation function:"
      ],
      "metadata": {
        "id": "0nl-zuw8mkeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = cVAE(input_dimension=97, latent_dim=4).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "classifier_criterion = nn.BCELoss()\n",
        "\n",
        "train_loss_list = []  # List to store train losses\n",
        "val_loss_list = []  # List to store validation losses\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1} of {num_epochs}\")\n",
        "    # Train the model\n",
        "    (   train_loss_autism,\n",
        "        train_loss_no_autism,\n",
        "        train_age_loss_autism,\n",
        "        train_gender_loss_autism,\n",
        "        train_age_loss_no_autism,\n",
        "        train_gender_loss_no_autism) = train(\n",
        "        model,\n",
        "        train_dataloader,\n",
        "        train_dataset,\n",
        "        device,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        classifier_criterion,\n",
        "    )\n",
        "\n",
        "    # Validate the model\n",
        "    val_loss_autism, val_loss_no_autism,\n",
        "        val_age_loss_autism, val_gender_loss_autism,\n",
        "        val_age_loss_no_autism, val_gender_loss_no_autism = validate(model, val_dataloader, val_dataset, device, criterion, classifier_criterion)\n",
        "\n",
        "    # Appending the loss values to a list to allow for visualizations:\n",
        "    train_loss_list.append(\n",
        "        train_loss_autism\n",
        "        + train_loss_no_autism\n",
        "        + train_age_loss_autism\n",
        "        + train_gender_loss_autism\n",
        "        + train_age_loss_no_autism\n",
        "        + train_gender_loss_no_autism\n",
        "    )\n",
        "    # val_loss_list.append(val_loss)\n",
        "\n",
        "    # Print the losses\n",
        "    print(\n",
        "        f\"Train Loss Autism: {train_loss_autism:.4f}, Train Loss Non-Autism: {train_loss_no_autism:.4f}, \"\n",
        "        f\"Train Age Loss Autism: {train_age_loss_autism:.4f}, Train Gender Loss Autism: {train_gender_loss_autism:.4f}, \"\n",
        "        f\"Train Age Loss Non-Autism: {train_age_loss_no_autism:.4f}, Train Gender Loss Non-Autism: {train_gender_loss_no_autism:.4f}\"\n",
        "    )\n",
        "    # print(f\"Train Loss for the background: {train_loss_bg:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "print(\"TRAINING COMPLETE\")\n"
      ],
      "metadata": {
        "id": "cgsEpXACmj_H",
        "outputId": "53f6b4aa-012f-44a4-c1a9-c8475f6acf53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9it [00:00, 49.19it/s]                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss Autism: 3.0942, Train Loss Non-Autism: 2.9689, Train Age Loss Autism: 2.8864, Train Gender Loss Autism: 0.0070, Train Age Loss Non-Autism: 2.8629, Train Gender Loss Non-Autism: 0.0068\n",
            "Epoch 2 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9it [00:00, 45.80it/s]                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss Autism: 2.9134, Train Loss Non-Autism: 2.8607, Train Age Loss Autism: 2.8247, Train Gender Loss Autism: 0.0065, Train Age Loss Non-Autism: 2.8137, Train Gender Loss Non-Autism: 0.0064\n",
            "Epoch 3 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9it [00:00, 49.46it/s]                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss Autism: 2.8056, Train Loss Non-Autism: 2.7804, Train Age Loss Autism: 2.7597, Train Gender Loss Autism: 0.0059, Train Age Loss Non-Autism: 2.7494, Train Gender Loss Non-Autism: 0.0060\n",
            "Epoch 4 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9it [00:00, 51.06it/s]                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss Autism: 2.6960, Train Loss Non-Autism: 2.6812, Train Age Loss Autism: 2.6564, Train Gender Loss Autism: 0.0053, Train Age Loss Non-Autism: 2.6507, Train Gender Loss Non-Autism: 0.0054\n",
            "Epoch 5 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9it [00:00, 56.08it/s]                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss Autism: 2.5903, Train Loss Non-Autism: 2.6011, Train Age Loss Autism: 2.5338, Train Gender Loss Autism: 0.0047, Train Age Loss Non-Autism: 2.5590, Train Gender Loss Non-Autism: 0.0049\n",
            "Epoch 6 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9it [00:00, 61.82it/s]                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss Autism: 2.4595, Train Loss Non-Autism: 2.4590, Train Age Loss Autism: 2.3620, Train Gender Loss Autism: 0.0043, Train Age Loss Non-Autism: 2.3910, Train Gender Loss Non-Autism: 0.0047\n",
            "Epoch 7 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9it [00:00, 91.88it/s]               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss Autism: 2.2445, Train Loss Non-Autism: 2.2818, Train Age Loss Autism: 2.0729, Train Gender Loss Autism: 0.0039, Train Age Loss Non-Autism: 2.1739, Train Gender Loss Non-Autism: 0.0041\n",
            "Epoch 8 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9it [00:00, 91.23it/s]               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss Autism: 1.9591, Train Loss Non-Autism: 2.0631, Train Age Loss Autism: 1.6783, Train Gender Loss Autism: 0.0038, Train Age Loss Non-Autism: 1.8949, Train Gender Loss Non-Autism: 0.0043\n",
            "Epoch 9 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9it [00:00, 101.46it/s]              \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss Autism: 1.7742, Train Loss Non-Autism: 1.8530, Train Age Loss Autism: 1.3892, Train Gender Loss Autism: 0.0038, Train Age Loss Non-Autism: 1.6112, Train Gender Loss Non-Autism: 0.0042\n",
            "Epoch 10 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9it [00:00, 97.78it/s]               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss Autism: 1.5570, Train Loss Non-Autism: 1.6691, Train Age Loss Autism: 1.0956, Train Gender Loss Autism: 0.0036, Train Age Loss Non-Autism: 1.3558, Train Gender Loss Non-Autism: 0.0040\n",
            "TRAINING COMPLETE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the optimal parameters - trial and error?"
      ],
      "metadata": {
        "id": "41F3o9-x6GwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Comparisons"
      ],
      "metadata": {
        "id": "2Uj2aiKi6K7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other models to compare:"
      ],
      "metadata": {
        "id": "IdZ6vdz56OVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions to Answer:\n",
        "How to best deal with with the issue that my data is in the shape of [batch size, 1] but the expected category against which its compared is [batch_size]\n",
        "\n",
        "Need to now prepare the validation data set to properly set the learning parameters."
      ],
      "metadata": {
        "id": "S5_MTGb5BCee"
      }
    }
  ]
}