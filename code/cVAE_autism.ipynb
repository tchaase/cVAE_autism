{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4Y9dMUqgTfsFem1S/Xzqe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tchaase/cVAE_autism/blob/main/code/cVAE_autism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contrastive Variational Autoencoder for the ABIDE Data Set\n",
        "\n",
        "Author - Tobias Haase"
      ],
      "metadata": {
        "id": "JrGziUQyNza-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "Firstly I am importaing the necessary modules here, that I will use within the following.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2OjCHUXODgN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHlOG4l2NsXd"
      },
      "outputs": [],
      "source": [
        "import torch  # The main PyTorch library for tensor computations and neural network operations\n",
        "\n",
        "import torch.nn as nn  # Provides various neural network layers and functionalities\n",
        "import torch.nn.functional as F  # Provides functional interfaces to common operations (e.g., activation functions)\n",
        "import torch.optim as optim  # Contains various optimization algorithms (e.g., SGD, Adam)\n",
        "\n",
        "import torchvision  # A PyTorch library for computer vision tasks\n",
        "import torchvision.transforms as transforms  # Provides common image transformations (e.g., resizing, normalization)\n",
        "from torchvision.transforms import ToTensor  # Transforms PIL images to tensors\n",
        "from torch.utils.data import Dataset, DataLoader  # Provides tools for creating custom datasets and data loaders\n",
        "\n",
        "import numpy as np  # NumPy library for numerical computations and array operations\n",
        "import matplotlib  # Matplotlib library for data visualization\n",
        "import matplotlib.pyplot as plt  # Matplotlib's pyplot module for creating plots\n",
        "from tqdm import tqdm  # Progress bar library for tracking iterations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's load the data. I am loading the data using nilearn's `fetch_abide_pcp` function. This function allows me to load the data that was previously preprocessed via the [preprocessed connectom project](http://preprocessed-connectomes-project.org/index.html) (PCP). Within this project, the data was preprocessed with four different pipelines.\n",
        "  \n",
        "\n",
        ">Due to the controversies surrounding bandpass filtering and global signal regression, four different preprocessing strategies were performed with each pipeline: all combinations of with and without filtering and with and without global signal correction.\n",
        "\n",
        "So, the first question to answer is which preprocessing pipeline I should take. Let's go over them step by step. I tried listing what data they focus on during the preprocessing. Then I want to briefly list key features that set them apart from other pipelines. Under dependencies I list mostly the dependencies they had during their usage, not what they require to load the data with!\n",
        "\n",
        "1. [Connectome Computation System](http://preprocessed-connectomes-project.org/abide/ccs.html):\n",
        "  * Preprocessing Steps: CCS involves the usual preprocessing steps, in which both the structural and functional data is preprocessed.\n",
        "  * Key Features: Perhaps it is important to note that this pipeline integrates FSL and Freesurfer and is primarily implemented using bash but also using various other programming languages.\n",
        "  * Dependencies: Therefore, this pipeline depends on FSL (skull stripping, normalization etc), freesurfer (e.g. anatomical segmentation, surface reconstruction) and AFNI (various preprocessing tools come from here)\n",
        "2. [Configurable Pipeline for the Analysis of Connectomes](http://preprocessed-connectomes-project.org/abide/cpac.html):\n",
        "    * Preprocessing Steps: CPAC incorporates a range of preprocessing steps for both structural and functional data. This includes motion correction, slice timing correction, spatial normalization, intensity normalization, nuisance signal regression, and band-pass filtering.\n",
        "    * Key Features: Most importantly, CPAC offers a high level of configuration as the name suggests. This allows the choice of several processing options based on their study requirements. It provides various quality control measures and outputs, including preprocessed functional connectivity matrices!\n",
        "    * Dependencies: CPAC is primarily implemented in Python and relies on various libraries and tools such as Nipype, FSL, ANTS, and AFNI.\n",
        "3. [Data Processing Assistant for Resting-State fMRI](http://preprocessed-connectomes-project.org/abide/dparsf.html):\n",
        "    * Preprocessing Steps: DPARSF focuses on resting-state functional MRI data and includes standard preprocessing steps such as slice timing correction, realignment (motion correction), spatial normalization, smoothing, and nuisance signal regression.\n",
        "    * Key Features: DPARSF provides a graphical user interface. There is a certian level of configurability, as ouput options can be choosen.\n",
        "    * Dependencies: DPARSF is implemented in MATLAB and requires SPM (Statistical Parametric Mapping) toolbox for some of the preprocessing steps.\n",
        "4. [Neuroimaging Analysis Kit](http://preprocessed-connectomes-project.org/abide/niak.html)\n",
        "    * Preprocessing Steps: NIAK allows customization of preprocessing steps, including motion correction, slice timing correction, spatial normalization, smoothing, and nuisance signal regression. It also offers quality control measures.\n",
        "    * Key Features: NIAK provides a flexible and versatile pipeline for functional and structural MRI data. It offers a command-line interface and the ability to select specific processing options based on the research requirements.\n",
        "    * Dependencies: NIAK is primarily implemented in MATLAB and relies on various external software packages such as FSL, ANTS, and AFNI for specific preprocessing steps.\n",
        "\n",
        "My sources for this information are both the website and ChatGPT.\n",
        "\n",
        "It seems to me that I can stick with the preset pipeline for now, which is **cpac**.\n",
        "\n",
        "Importantly, quality control was already performed for this data, and I will only load the data that has gone through the quality control successfully.\n",
        "\n",
        "For now, I am just loading one participant."
      ],
      "metadata": {
        "id": "k_7TkkoWPkfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nilearn.datasets.fetch_abide_pcp(data_dir = \"./data\", n_subjects = 1)"
      ],
      "metadata": {
        "id": "nap5ytiEPnBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model specifications\n",
        "\n",
        "In the following I am specifiying the model. I am roughly orienting myself around a paper from Anglinkas, Hartshorne & Anzellotti (2022).\n",
        "\n",
        "### Defining utility functions\n",
        "\n",
        "Firstly, I am defining the loss function.\n",
        "The loss will be computed as the sum of the BCE-Loss, as well as the KL-divergence terms.\n",
        "\n",
        "* BCE-loss: Reconstruction loss. This is the binary cross entropy, i.e. a loss function that is normally used when making a binary classification. Using it here as a placeholder, until I find a more appropriate loss function. If a prediction is incorrect, it has high values.\n",
        "\n",
        "* Kullback-Leibler divergence (Kullback & Leibler, 1951) This is a measure for the difference between two distributions. I.e. \"how much do they diverge\" from each other, how much are they different to each other. The introduction of this term into the final loss leads my model to optimize not only if the precited categories are correct and so on, but also how high the difference between the prior distribution and teh latent variables are. The prior distribution in my case is an isotropic gaussian.\n",
        "  * Why is this desirable? The latent variables and the sampling process should be somewhat controlled. This divergence regulates this.\n",
        "\n",
        "\n",
        "I have also attempted to regulate that a loss is only completed with the KL divergence from the second encoder if that encoder was used."
      ],
      "metadata": {
        "id": "NV2DSYrow9-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_loss(bce_loss, z_mu, z_logvar, s_mu=None, s_logvar=None):\n",
        "    \"\"\"\n",
        "    This function will add the reconstruction loss (BCELoss) and the KL-Divergence.\n",
        "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    :param bce_loss: reconstruction loss\n",
        "    :param z_mu: mean from the latent vector of encoder_z\n",
        "    :param z_logvar: log variance from the latent vector of encoder_z\n",
        "    :param s_mu: mean from the latent vector of encoder_s (optional)\n",
        "    :param s_logvar: log variance from the latent vector of encoder_s (optional)\n",
        "    \"\"\"\n",
        "    BCE = bce_loss\n",
        "    KLD_z = -0.5 * torch.sum(1 + z_logvar - z_mu.pow(2) - z_logvar.exp())\n",
        "    if s_mu is not None and s_logvar is not None:\n",
        "        KLD_s = -0.5 * torch.sum(1 + s_logvar - s_mu.pow(2) - s_logvar.exp())\n",
        "        return BCE + KLD_z + KLD_s\n",
        "    else:\n",
        "        return BCE + KLD_z"
      ],
      "metadata": {
        "id": "pl9SJ8uDyDuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next onto the training function. This should be fairly straightforward."
      ],
      "metadata": {
        "id": "AqGCBLjH5oyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, dataset, device, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    counter = 0\n",
        "    for i, (data, is_autism) in tqdm(enumerate(dataloader), total=len(dataset)):\n",
        "        data = data.to(device)\n",
        "        autism_status = is_autism.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for j in range(data.size(0)):\n",
        "            single_data = data[j]\n",
        "            single_autism_status = autism_status[j]\n",
        "\n",
        "            if single_autism_status:\n",
        "                z_mean, z_log_var, s_mean, s_log_var, reconstructed_data = model(single_data)\n",
        "                bce_loss = criterion(reconstructed_data, single_data)\n",
        "                loss = final_loss(bce_loss, z_mean, z_log_var, s_mean, s_log_var)\n",
        "            else:\n",
        "                z_mean, z_log_var, reconstructed_data = model(single_data)\n",
        "                bce_loss = criterion(reconstructed_data, single_data)\n",
        "                loss = final_loss(bce_loss, z_mean, z_log_var, None, None)\n",
        "\n",
        "            loss.backward()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        counter += data.size(0)\n",
        "\n",
        "    train_loss = running_loss / counter\n",
        "    return train_loss"
      ],
      "metadata": {
        "id": "Nc6vo9Kn58gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following this, the model should also be validated. This is placeholder code for now (also didn't finish the training function)."
      ],
      "metadata": {
        "id": "VTFPxBey6Mmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataloader, dataset, device, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    counter = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n",
        "            counter += 1\n",
        "            data = data[0]\n",
        "            data = data.to(device)\n",
        "            reconstruction, z_mu, z_logvar, s_mu, s_logvar = model(data)\n",
        "            bce_loss = criterion(reconstruction, data)\n",
        "            loss = final_loss(bce_loss, z_mu, z_logvar, s_mu, s_logvar)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # save the last batch input and output of every epoch\n",
        "            if i == int(len(dataset)/dataloader.batch_size) - 1:\n",
        "                recon_images = reconstruction\n",
        "    val_loss = running_loss / counter\n",
        "    return val_loss, recon_images\n"
      ],
      "metadata": {
        "id": "-KnJFnNl6UCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model specification\n",
        "\n",
        "These values still need to be adapted for the current model."
      ],
      "metadata": {
        "id": "rtNUqWlK6YfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_channels = 64 # initial number of filters, first layers output.\n",
        "image_channels = 1 # MNIST images are grayscale\n",
        "latent_dim = 16 # latent dimension for sampling\n",
        "filters\n",
        "\n",
        "kernel_size = 3\n",
        "stride = 2\n",
        "same = 0\n",
        "padding = same\n",
        "\n",
        "lr = 0.001\n",
        "\n",
        "indermediate_dim = 128"
      ],
      "metadata": {
        "id": "1NXSPtSc6biA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next I want to define the contrastive variational autoencoder. While doing so, I am defining seperate encoders, to make it easier to later introduce other encoders. I am orienting myself on an cVAE I have written in the past.\n",
        "\n",
        "As the paper from Aglinskas, Hartshorne and Anzellotti (2022) I mentioned, the network will have only a few layers.\n",
        "\n",
        "A few things I will probably have to change - I do not know how many channels the data will end up having. therefore I am using one, assuming it only has one."
      ],
      "metadata": {
        "id": "yzEoxSr08h-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderNS(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(EncoderNS, self).__init__()\n",
        "        self.shared_conv1 = nn.Conv3d(in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.shared_conv2 = nn.Conv3d(in_channels=init_channels, out_channels=init_channels * 2, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.linear = nn.Linear(None, intermediate_dim)\n",
        "        self.ns_fc_mean = nn.Linear(intermediate_dim, latent_dim)\n",
        "        self.ns_fc_log_var = nn.Linear(intermediate_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.shared_conv1(x))\n",
        "        h = F.relu(self.shared_conv2(h))\n",
        "        h = h.view(h.size(0), -1)\n",
        "        h = F.relu(self.linear(h))\n",
        "        ns_mean = self.ns_fc_mean(h)\n",
        "        ns_log_var = self.ns_fc_log_var(h)\n",
        "        return ns_mean, ns_log_var\n",
        "\n",
        "\n",
        "class EncoderS(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(EncoderS, self).__init__()\n",
        "        self.specific_conv1 = nn.Conv3d(in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.specific_conv2 = nn.Conv3d(in_channels=init_channels, out_channels=init_channels * 2, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.linear = nn.Linear(None, intermediate_dim)\n",
        "        self.s_fc_mean = nn.Linear(intermediate_dim, latent_dim)\n",
        "        self.s_fc_log_var = nn.Linear(intermediate_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.specific_conv1(x))\n",
        "        h = F.relu(self.specific_conv2(h))\n",
        "        h = h.view(h.size(0), -1)\n",
        "        h = F.relu(self.linear(h))\n",
        "        s_mean = self.s_fc_mean(h)\n",
        "        s_log_var = self.s_fc_log_var(h)\n",
        "        return s_mean, s_log_var\n",
        "\n",
        "\n",
        "class Decoder(nn.Module)\n",
        "    def __init__(self, latent_dim)\n",
        "      super(Decoder, self).__init__()\n",
        "      self.linear_decoder_1 = nn.Linear(latent_dim, intermediate_dim)\n",
        "      self.linear_decoder_2 = nn.Linear(#? dont know yet depends if we want to get 3d input and what size)\n",
        "      self.conv_decoder_1 = nn.ConvTranspose2d(in_channels = filters, out_channels = filters,  kernel_size = kernel_size, stride = stride, padding = padding)\n",
        "      self.conv_decoder_2 = nn.ConvTranspode2d(in channels = filters, out_channels = 1, kernel_size = kernel_size, stirde = stride, padding = padding)\n",
        "\n",
        "    def forward(self, s_ns)\n",
        "      h_output = F.relu(self.linear_decoder_1(s_ns))\n",
        "      h_output = F.relu(self.linear_decoder_2(h_output))\n",
        "      h_output = nn.Unflatten(1, #depends on what we work with)\n",
        "      h_output = F.relu(self.conv_decoder_1(h_output))\n",
        "      output = F.Sig(self.conv_decoder_2(h_output)) #choice of activation function depends on my choice of a loss function\n",
        "      return output\n",
        "\n",
        "\n",
        "class cVAE(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(cVAE, self).__init__()\n",
        "        self.encoder_ns = EncoderNS(latent_dim)\n",
        "        self.encoder_s = EncoderS(latent_dim)\n",
        "        self.decoder = Decoder(latent_dim)\n",
        "        self.autism = None\n",
        "\n",
        "\n",
        "    def reparameterize(self, mean, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        epsilon = torch.randn_like(std)\n",
        "        return mean + epsilon * std\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        ns_mean, ns_log_var = self.encoder_ns(x)\n",
        "        ns = self.reparameterize(ns_mean, ns_log_var)\n",
        "\n",
        "        if self.autism:\n",
        "            s_mean, s_log_var = self.encoder_s(x)\n",
        "            s = self.reparameterize(s_mean, s_log_var)\n",
        "            s_ns = torch.cat([ns, s], dim=1)\n",
        "        else:\n",
        "            empty_vector = torch.empty_like(ns).detach()\n",
        "            s_ns = torch.cat([ns,empty_vector], dim = 1)\n",
        "\n",
        "        reconstructed_data = self.decoder(s_ns)\n",
        "\n",
        "        if self.autism:\n",
        "            return ns_mean, ns_log_var, s_mean, s_log_var, reconstructed_data\n",
        "        else:\n",
        "            return ns_mean, ns_log_var, reconstructed_data\n"
      ],
      "metadata": {
        "id": "1_d_NMA88Mhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upcoming: I still need to define the training and validation function and all of the analyses.\n"
      ],
      "metadata": {
        "id": "k9lxR8HrHEaQ"
      }
    }
  ]
}